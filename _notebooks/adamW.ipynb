{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding L2 regularization, Weight decay and AdamW\n",
    "> A post explaining L2 regularization, Weight decay and AdamW optimizer as described in the paper Decoupled Weight Decay Regularization we will also go over how to implement these using tensorflow2.x .\n",
    "\n",
    "- toc: false\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [machinelearning deeplearning python3.x tensorflow2.x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is regularization ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple words regularization helps in reduces over-fitting on the data. There are many regularization strategies.\n",
    "\n",
    "The major regularization techniques used in practice are:\n",
    "* L2 Regularization\n",
    "* L1 Regularization\n",
    "* Data Augmentation\n",
    "* Dropout\n",
    "* Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 regularization :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In L2 regularization, an extra term often referred to as regularization term is added to the loss function of the network.\n",
    " \n",
    "Consider the the following cross entropy loss function (without regularization): \n",
    "\n",
    "$$loss= -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(yhat^{(i)}\\right) + (1-y^{(i)})\\log\\left(1-yhat^{(i)}\\right)) $$\n",
    "\n",
    "To apply L2 regularization to the loss function above we add the term given below to the loss function :\n",
    "\n",
    "$$\\frac{\\lambda}{2m}\\sum\\limits_{w}w^{2} $$\n",
    "\n",
    "where $\\lambda$ is a hyperparameter of the model known as the regularization parameter. $\\lambda$ is a hyper-parameter which means it is not learned during the training but is tuned by the user manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the `regularization term` to our original loss function :\n",
    "$$finalLoss= -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(yhat^{(i)}\\right) + (1-y^{(i)})\\log\\left(1-yhat^{(i)}\\right)) + \\frac{\\lambda}{2m}\\sum\\limits_{w}w^{2}$$\n",
    "\n",
    "or , \n",
    "$$ finalLoss = loss+ \\frac{\\lambda}{2m}\\sum\\limits_{w}w^{2}$$\n",
    "\n",
    "\n",
    "or in simple code :\n",
    "\n",
    "\n",
    "```python\n",
    "final_loss = loss_fn(y, y_hat) + lamdba * np.sum(np.pow(weights, 2)) / 2\n",
    "final_loss = loss_fn(y, y_hat) + lamdba * l2_reg_term \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: all code equations are written in python, numpy notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosequently the weight update step for **vanilla SGD** is going to look something like this:\n",
    "```python\n",
    "w = w - learning_rate * grad_w - learning_rate * lamdba * grad(l2_reg_term, w)\n",
    "w = w - learning_rate * grad_w - learning_rate * lamdba * w\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: assume that grad_w is the gradients of the loss of the model wrt weights of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: assume that grad(a,b) calculates the gradients of a wrt to b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Decay :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In weight decay we do not modify the loss function, the loss function remains the instead instead we modfy the update step :\n",
    "\n",
    "\n",
    "The loss remains the same :\n",
    "\n",
    "```python\n",
    "final_loss = loss_fn(y, y_hat)\n",
    "```\n",
    "\n",
    "During the update parameters :\n",
    "\n",
    "```python\n",
    "w = w - learing_rate * grad_w - learning_rate * lamdba * w\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Important: From the above equations **weight_decay** and **L2 regularization** may seem the same and it is infact same for **vanilla SGD** , but as soon as we add momentum, or use a more sophisticated optimizer **like Adam**, **L2 regularization** and **weight decay** become different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
