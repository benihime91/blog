{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020-11-02-tensorflow-object-detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOyfcta16nDmdc1sNTDGjiJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benihime91/blog/blob/master/_notebooks/2020-11-02-tensorflow-object-detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlHHd8rX8CLt"
      },
      "source": [
        "# TensorFlow Object Detection API Tutorial\n",
        "> TensorFlow recently announced [TF Object Detection API models to be TensorFlow 2 compatible](https://blog.tensorflow.org/2020/07/tensorflow-2-meets-object-detection-api.html) . In this tutorial we will go over on how to train a object detection model on custom dataset using TensorFlow Object Detection API 2.\n",
        "\n",
        "- toc: false\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [machinelearning deeplearning tensorflow2.x tensorflow-object-detection]\n",
        "- image: images/object-detection.jpg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwfw_ORfE1nu",
        "outputId": "6ae4f159-5a65-4c1e-9a38-616f0c05e7b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#hide\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Nov  3 09:38:10 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALR2laa9PYpa"
      },
      "source": [
        "> Important: This guide is based on [official tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html) and could intersect with this [Tutorial from Roboflow-Team]((https://blog.roboflow.com/train-a-tensorflow2-object-detection-model/). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCCuhf1kmv3Q"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "In this notebook, we implement [The TensorFlow 2 Object Detection Library](https://blog.tensorflow.org/2020/07/tensorflow-2-meets-object-detection-api.html) for training on your own dataset.\n",
        "\n",
        "\n",
        "We will take the following steps to implement a model from **[TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)** on our custom data:\n",
        "* Install TensorFlow2 Object Detection Dependencies\n",
        "* Download Custom TensorFlow2 Object Detection Dataset\n",
        "* Write Custom TensorFlow2 Object Detection Training Configuation\n",
        "* Train Custom TensorFlow2 Object Detection Model\n",
        "* Export Custom TensorFlow2 Object Detection Weights\n",
        "* Use Trained TensorFlow2 Object Detection For Inference on Test Images\n",
        "\n",
        "When you are done you will have a custom detector that you can use. It will make inference like this:\n",
        "\n",
        "![](https://www.dropbox.com/s/slkbti9incj3k09/object-detection.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xEeVSgmoL6U"
      },
      "source": [
        "#Install TensorFlow2 Object Detection Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0X5ZEE1OyiR"
      },
      "source": [
        "To install **TensorFlow2 Object Detection** on Google-Colab run the following steps. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfNqKMd7YXAe",
        "outputId": "0b719734-96af-433a-9867-4693a59d76bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#hide_output\n",
        "import os\n",
        "import pathlib\n",
        "\n",
        "# Clone the tensorflow models repository if it doesn't already exist\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 2281, done.\u001b[K\n",
            "remote: Counting objects: 100% (2281/2281), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1977/1977), done.\u001b[K\n",
            "remote: Total 2281 (delta 556), reused 955 (delta 278), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2281/2281), 30.55 MiB | 29.24 MiB/s, done.\n",
            "Resolving deltas: 100% (556/556), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKS6DbhpYgo4"
      },
      "source": [
        "#hide_output\n",
        "\n",
        "# Install the Object Detection API\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK4R-kbYo2vK"
      },
      "source": [
        "Run the TF2 model builder tests to make sure our environment is up and running. If successful  If successful, you should see the following outputs at the end of the cell execution printouts.\n",
        "\n",
        "```\n",
        "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
        "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
        "----------------------------------------------------------------------\n",
        "Ran 20 tests in 52.705s\n",
        "\n",
        "OK (skipped=1)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36KcetmMcEzh",
        "outputId": "c51fd809-3285-43c3-dcca-86a601934e18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#hide_output\n",
        "\n",
        "#run model builder test to \n",
        "!python /content/models/research/object_detection/builders/model_builder_tf2_test.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-03 09:25:31.726670: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/models/research/object_detection/builders/model_builder_tf2_test.py\", line 24, in <module>\n",
            "    from object_detection.builders import model_builder\n",
            "ModuleNotFoundError: No module named 'object_detection'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-EWBu4eO94b"
      },
      "source": [
        "To install on a custom machine check : [Installation](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_xmd22ToUsM"
      },
      "source": [
        "# Download the data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEiOFcHwoc-q"
      },
      "source": [
        "For this task we are going to be using the Oxford Pets dataset. This dataset contains 37 category pet dataset with roughly 200 images for each class. The annotations contain tight bounding box (ROI) around the head of the animal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd1OXlyjXIPl",
        "outputId": "bff68b57-d124-4b48-aadf-b1d7e3cd92e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#hide_output\n",
        "\n",
        "#Download the Oxford-IIIT Pet \n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "!tar -xf annotations.tar.gz\n",
        "!tar -xf images.tar.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-03 09:54:04--  http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 791918971 (755M) [application/x-gzip]\n",
            "Saving to: ‘images.tar.gz’\n",
            "\n",
            "images.tar.gz       100%[===================>] 755.23M  22.2MB/s    in 35s     \n",
            "\n",
            "2020-11-03 09:54:39 (21.8 MB/s) - ‘images.tar.gz’ saved [791918971/791918971]\n",
            "\n",
            "--2020-11-03 09:54:39--  http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19173078 (18M) [application/x-gzip]\n",
            "Saving to: ‘annotations.tar.gz’\n",
            "\n",
            "annotations.tar.gz  100%[===================>]  18.28M  10.7MB/s    in 1.7s    \n",
            "\n",
            "2020-11-03 09:54:41 (10.7 MB/s) - ‘annotations.tar.gz’ saved [19173078/19173078]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi5O2wG-7vUx",
        "outputId": "0bf6735d-e150-462d-9f03-047a0e5477a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#hide\n",
        "!apt-get install tree"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 11 not upgraded.\n",
            "Need to get 40.7 kB of archives.\n",
            "After this operation, 105 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 40.7 kB in 1s (64.6 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 144628 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Msx5JA7Ahz5O"
      },
      "source": [
        "Before training let us create a folder **/content/workspace/**. \n",
        "\n",
        "It is within the **workspace** that we will store all our training set-ups. This will contain all files related to our model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwjV7RPIji4L"
      },
      "source": [
        "#Create the directory structure\n",
        "#We will store all the required files in the workspace folder\n",
        "!mkdir /content/workspace/\n",
        "!mkdir /content/workspace/images/ # store images\n",
        "!mkdir /content/workspace/annotations/ # store xml annotation files\n",
        "!mkdir /content/workspace/images/train # train images\n",
        "!mkdir /content/workspace/images/test # test images\n",
        "!mkdir /content/workspace/annotations/train # train annotations\n",
        "!mkdir /content/workspace/annotations/test # test annotations\n",
        "!mkdir /content/workspace/data/ # directory to store the tf_records & the label_map"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y99chLZhirGO"
      },
      "source": [
        "Our directory at this stage should look like this :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjwM6522iCxI",
        "outputId": "7bf70d7b-7a53-421b-9504-ef19089d96d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#hide_input\n",
        "!tree -C \"/content/workspace\" --filelimit=100"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[01;34m/content/workspace\u001b[00m\n",
            "├── \u001b[01;34mannotations\u001b[00m\n",
            "│   ├── \u001b[01;34mtest\u001b[00m\n",
            "│   └── \u001b[01;34mtrain\u001b[00m\n",
            "├── \u001b[01;34mdata\u001b[00m\n",
            "└── \u001b[01;34mimages\u001b[00m\n",
            "    ├── \u001b[01;34mtest\u001b[00m\n",
            "    └── \u001b[01;34mtrain\u001b[00m\n",
            "\n",
            "7 directories, 0 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sil1Sw2xj-Lr"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "import logging\n",
        "import re\n",
        "import shutil\n",
        "import glob\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tarfile\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "IMAGE_DIR = \"/content/images\"\n",
        "ANNOT_DIR = \"/content/annotations/xmls\"\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "os.chdir(\"/content/\")\n",
        "\n",
        "%load_ext tensorboard\n",
        "%load_ext autoreload\n",
        "%matplotlib inline\n",
        "%autoreload 2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kQUk7wSvwUI"
      },
      "source": [
        "import tensorflow.compat.v1 as tf1\n",
        "import contextlib2\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.utils import dataset_util\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import colab_utils\n",
        "from object_detection.builders import model_builder\n",
        "from object_detection.dataset_tools import tf_record_creation_util\n",
        "\n",
        "\n",
        "# Enable GPU dynamic memory allocation\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "#Suppress TensorFlow logging\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdQxI-c3pqNX"
      },
      "source": [
        "#Prepare Tensorflow 2 Object Detection Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH_CzhmQprdu"
      },
      "source": [
        "Tensorflow object detection API expects the data to be in the form of **TFRecords** . In this part we are going to convert our data present in **Pascal-VOC** format into **TFRecords**.  \n",
        "\n",
        "To do this we will implement the following the steps:\n",
        "\n",
        "* Iterate over all the annotations and partition the annotations into train and test datasets. The train annotatins and images will be saved to **/content/workspace/annotations/train** & **/content/workspace/images/train** respectively. Similarly the test data will be saved to **/content/workspace/annotations/test** & **/content/workspace/images/test** .\n",
        "\n",
        "* Convert all the `*.xml` annotation files into a single Pandas DataFrame object.\n",
        "\n",
        "* We will create a tensoflow 2 object detection format label-map which will be used in training/evaluation the model .\n",
        "\n",
        "* Use this Pandas DataFrame to create **TFRecords** for the train and test datasets. The **TFRecords** will be saved to **/content/workspace/data/**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WmTbGMPsH8J"
      },
      "source": [
        "## 1. **Partition the Dataset** :\n",
        "\n",
        "If we look at the data that is saved in **/content/images/** & **/content/annotations/** we will see that not all the images have the corresponding annotations and the images and annotations are saved as **{filename}.jpeg** & **{filename}.xml** respectively.\n",
        "\n",
        "What we will do is we will first split the images using `sklearn.train_test_split` into a train and test dataset. Then we will check for the corresponding annotation for the image . If the annotation file exists we will copy the image and annotation into their repectives directories under **/content/workspace** ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da9sXK5RkF7F"
      },
      "source": [
        "#Grab the list of all images\n",
        "all_images    = os.listdir(IMAGE_DIR)\n",
        "\n",
        "#Split the images into train and test datasets\n",
        "train_images, test_images = train_test_split(all_images, test_size=0.2, random_state = 123) \n",
        "\n",
        "#Grab the list of all the annotations for the train and test images\n",
        "#Some annotations may not exist we will filter these in the next cell\n",
        "train_xmls = [f.split(\".\")[0] + \".xml\" for f in train_images]\n",
        "test_xmls  = [f.split(\".\")[0] + \".xml\" for f in test_images ]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrgMBm63nK07",
        "outputId": "5c79a626-e092-4670-db39-da57c8172342",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def move_file(fileList : list, src: str, dest: str):\n",
        "    \"\"\"\n",
        "    This Fn copy's files from a given fileList from src to dest\n",
        "    if the file exits.\n",
        "\n",
        "    Args:\n",
        "        fileList: List containing all the files present in the src directory.\n",
        "        src     : source directory for the files.\n",
        "        dest    : destination where to copy the files present in fileList.\n",
        "    \"\"\"\n",
        "    for f in tqdm(fileList):\n",
        "        fileName = os.path.join(src, f)\n",
        "        #Check if the file exits, if the file exits copy contents from src to dest\n",
        "        if os.path.exists(fileName):\n",
        "            shutil.copy2(src=fileName, dst=os.path.join(dest, f))\n",
        "\n",
        "\n",
        "\n",
        "#Move images and annotations to workspace directory\n",
        "move_file(train_images, src=IMAGE_DIR, dest=\"/content/workspace/images/train/\")\n",
        "move_file(test_images,  src=IMAGE_DIR, dest=\"/content/workspace/images/test/\")\n",
        "\n",
        "move_file(train_xmls, src=ANNOT_DIR, dest=\"/content/workspace/annotations/train/\")\n",
        "move_file(test_xmls,  src=ANNOT_DIR, dest=\"/content/workspace/annotations/test/\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5914/5914 [00:01<00:00, 2962.49it/s]\n",
            "100%|██████████| 1479/1479 [00:00<00:00, 2204.23it/s]\n",
            "100%|██████████| 5914/5914 [00:00<00:00, 14943.15it/s]\n",
            "100%|██████████| 1479/1479 [00:00<00:00, 17687.80it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c4skLSXuuBf"
      },
      "source": [
        "## 2. **Create Pandas DataFrame Object** :\n",
        "\n",
        "Now, that we have partitioned our dataset and the images/annotations are present in the repective directories we will now create a pandas dataframe from the `*.xml` files . The DataFrame will contrain the fillowing information:\n",
        "* **`filename`** `(str)`: Path to the image file.\n",
        "* **`width`**\t `(float/int)`: Absolute width of the image.\n",
        "* **`height`** `(float/int)`: Absolute height of the image.\n",
        "* **`labels`** `(str)`: The class of the object present in the bounding box.\t\n",
        "* **`xmin`** `(float/int)`: Absolute `xmin` co-ordinate for the bounding box.\n",
        "* **`ymin`** `(float/int)`: Absolute `ymin` co-ordinate for the bounding box.\n",
        "* **`xmax`** `(float/int)`: Absolute `xmax` co-ordinate for the bounding box.\t\n",
        "* **`ymax`** `(float/int)`: Absolute `ymax` co-ordinate for the bounding box.\n",
        "* **`encoded_label`** `(int)`: The label for the object in the bounding box. 0 represents always the background class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ujBfJs9YOy0"
      },
      "source": [
        "#Regular expression which will help us to extract the class of the object in the image\n",
        "#from the filename.\n",
        "exp = r\"/([^/]+)_\\d+.jpg$\"\n",
        "exp = re.compile(exp)\n",
        "\n",
        "#sklearn.LabelEncoder will be used to convert the class of the object into integer format.\n",
        "le  = LabelEncoder()\n",
        "\n",
        "def xml2pandas(annot_dir):\n",
        "    \"\"\"\n",
        "    Fn converts the xml files into a pandas dataframe.\n",
        "\n",
        "    Args:\n",
        "        annot_dir: Directory where all the *.xml annotation files are stored\n",
        "    \"\"\"\n",
        "    xml_list = []\n",
        "    for xml_file in tqdm(glob.glob(annot_dir + '/*.xml')):\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        for member in root.findall('object'):\n",
        "            value = (\n",
        "                root.find('filename').text,\n",
        "                int(root.find('size')[0].text),\n",
        "                int(root.find('size')[1].text),\n",
        "                member[0].text,\n",
        "                int(member[4][0].text),\n",
        "                int(member[4][1].text),\n",
        "                int(member[4][2].text),\n",
        "                int(member[4][3].text)\n",
        "                )\n",
        "            xml_list.append(value)\n",
        "        column_name = ['filename', 'width', 'height','labels', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "        xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
        "    logging.info(\"DataFrame Generated ! \")\n",
        "    return xml_df\n",
        "\n",
        "\n",
        "def process_data(annotDir, imageDir, image_set=\"train\"):\n",
        "    \"\"\"\n",
        "    Fn creates a pandas DataFrame object from the annotation in annotDir\n",
        "    and images in imageDir. This Fn also extracts the name of the class from the\n",
        "    filename and converts it into integer labels starting from 1 as 0 is reserved\n",
        "    for the background class always.\n",
        "\n",
        "    Args:\n",
        "        annotDir  : directory where the *.xml annotation files are stored.\n",
        "        imageDir  : directory where all the images are stored.\n",
        "        image_set : one of either `train` or `test`, this use when converting \n",
        "                    the class objects into integer formats.\n",
        "    \"\"\"\n",
        "    data = xml2pandas(annotDir)\n",
        "    #modify the filename to point to the original filename\n",
        "    data.filename = [os.path.join(imageDir, fname) for fname in data.filename.values]\n",
        "    #extract the class labels from the filenames\n",
        "    data[\"labels\"] = [exp.search(data.filename[idx]).group(1).lower() for idx in range(len(data))]\n",
        "    #encoded the labels into integers starting from 1\n",
        "    if image_set == \"train\" :\n",
        "        data[\"encoded_label\"] = le.fit_transform(data.labels) + 1\n",
        "    elif image_set == \"test\" :\n",
        "        data[\"encoded_label\"] = le.transform(data.labels) + 1\n",
        "    \n",
        "    return data"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USZgAbpmZRsC",
        "outputId": "16d1ecb7-8065-4b6d-b812-f9d2c2e0f46c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Set up paths the train/test image and annotaion directory\n",
        "TRAIN_IMAGE_DIR = \"/content/workspace/images/train/\"\n",
        "TEST_IMAGE_DIR  = \"/content/workspace/images/test/\"\n",
        "TRAIN_ANNOTATION_DIR = \"/content/workspace/annotations/train/\"\n",
        "TEST_ANNOTATION_DIR  = \"/content/workspace/annotations/test/\"\n",
        "\n",
        "#Create pandas datafame from the *.xml files\n",
        "train_data = process_data(TRAIN_ANNOTATION_DIR, TRAIN_IMAGE_DIR, \"train\")\n",
        "test_data  = process_data(TEST_ANNOTATION_DIR, TEST_IMAGE_DIR, \"test\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2958/2958 [00:10<00:00, 289.82it/s]\n",
            "100%|██████████| 729/729 [00:01<00:00, 574.27it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaJPqQk7I7P8",
        "outputId": "e959448a-510c-4fba-912e-690e103aca9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Cross check for missing files\n",
        "for f in train_data.filename:\n",
        "    if not os.path.exists(f):\n",
        "        #remove the missing file\n",
        "        print(f\"{f} is missing in train_data\")\n",
        "        train_data = train_data[train_data.filename != f]\n",
        "        train_data.reset_index(inplace=True, drop=True)\n",
        "\n",
        "\n",
        "for f in test_data.filename:\n",
        "    if not os.path.exists(f):\n",
        "        #remove the missing file\n",
        "        print(f\"{f} missing in test_data\")\n",
        "        test_data = test_data[test_data.filename != f]\n",
        "        test_data.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/workspace/images/test/Abyssinian_101.jpg missing in test_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G45XOUpayHN"
      },
      "source": [
        "**Our datasets are going to look something like this :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMOICsnka2vg"
      },
      "source": [
        "**The train_data :** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anGyy0wtHNh2",
        "outputId": "c4cfa8cb-5bcd-4b5f-ff15-8d108500382d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#hide_input\n",
        "\n",
        "#view the train dataset\n",
        "train_data.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>labels</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "      <th>encoded_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/workspace/images/train/keeshond_102.jpg</td>\n",
              "      <td>500</td>\n",
              "      <td>449</td>\n",
              "      <td>keeshond</td>\n",
              "      <td>281</td>\n",
              "      <td>65</td>\n",
              "      <td>387</td>\n",
              "      <td>202</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/workspace/images/train/basset_hound_153.jpg</td>\n",
              "      <td>500</td>\n",
              "      <td>375</td>\n",
              "      <td>basset_hound</td>\n",
              "      <td>138</td>\n",
              "      <td>32</td>\n",
              "      <td>357</td>\n",
              "      <td>345</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/workspace/images/train/Egyptian_Mau_101.jpg</td>\n",
              "      <td>480</td>\n",
              "      <td>360</td>\n",
              "      <td>egyptian_mau</td>\n",
              "      <td>320</td>\n",
              "      <td>85</td>\n",
              "      <td>391</td>\n",
              "      <td>170</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/workspace/images/train/newfoundland_150.jpg</td>\n",
              "      <td>377</td>\n",
              "      <td>500</td>\n",
              "      <td>newfoundland</td>\n",
              "      <td>147</td>\n",
              "      <td>37</td>\n",
              "      <td>358</td>\n",
              "      <td>255</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/workspace/images/train/beagle_102.jpg</td>\n",
              "      <td>333</td>\n",
              "      <td>500</td>\n",
              "      <td>beagle</td>\n",
              "      <td>90</td>\n",
              "      <td>99</td>\n",
              "      <td>242</td>\n",
              "      <td>206</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               filename  ...  encoded_label\n",
              "0      /content/workspace/images/train/keeshond_102.jpg  ...             19\n",
              "1  /content/workspace/images/train/basset_hound_153.jpg  ...              4\n",
              "2  /content/workspace/images/train/Egyptian_Mau_101.jpg  ...             12\n",
              "3  /content/workspace/images/train/newfoundland_150.jpg  ...             23\n",
              "4        /content/workspace/images/train/beagle_102.jpg  ...              5\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHnhaFQJa50t"
      },
      "source": [
        "**The test_data :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGme3RrgmaZj",
        "outputId": "392a16aa-8d04-4eca-c74f-45a3b659bf52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#hide_input\n",
        "\n",
        "#view the test dataset\n",
        "test_data.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>labels</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "      <th>encoded_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/workspace/images/test/boxer_117.jpg</td>\n",
              "      <td>500</td>\n",
              "      <td>333</td>\n",
              "      <td>boxer</td>\n",
              "      <td>205</td>\n",
              "      <td>25</td>\n",
              "      <td>318</td>\n",
              "      <td>113</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/workspace/images/test/pug_10.jpg</td>\n",
              "      <td>333</td>\n",
              "      <td>500</td>\n",
              "      <td>pug</td>\n",
              "      <td>70</td>\n",
              "      <td>37</td>\n",
              "      <td>195</td>\n",
              "      <td>133</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/workspace/images/test/Sphynx_203.jpg</td>\n",
              "      <td>332</td>\n",
              "      <td>500</td>\n",
              "      <td>sphynx</td>\n",
              "      <td>109</td>\n",
              "      <td>173</td>\n",
              "      <td>205</td>\n",
              "      <td>274</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/workspace/images/test/Bengal_127.jpg</td>\n",
              "      <td>500</td>\n",
              "      <td>372</td>\n",
              "      <td>bengal</td>\n",
              "      <td>115</td>\n",
              "      <td>14</td>\n",
              "      <td>204</td>\n",
              "      <td>122</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/workspace/images/test/boxer_123.jpg</td>\n",
              "      <td>500</td>\n",
              "      <td>333</td>\n",
              "      <td>boxer</td>\n",
              "      <td>205</td>\n",
              "      <td>185</td>\n",
              "      <td>241</td>\n",
              "      <td>231</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        filename  width  ...  ymax encoded_label\n",
              "0   /content/workspace/images/test/boxer_117.jpg    500  ...   113             9\n",
              "1      /content/workspace/images/test/pug_10.jpg    333  ...   133            26\n",
              "2  /content/workspace/images/test/Sphynx_203.jpg    332  ...   274            34\n",
              "3  /content/workspace/images/test/Bengal_127.jpg    500  ...   122             6\n",
              "4   /content/workspace/images/test/boxer_123.jpg    500  ...   231             9\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqGEDw_uy9SA"
      },
      "source": [
        "## 3. **Create Label Map** :\n",
        "\n",
        "TensorFlow requires dataset to have a label map associated with it. This label map defines a mapping from string class names to integer class Ids. The label map should be a StringIntLabelMap text protobuf. Label map files have the extention `.pbtxt` and  we will place it under **/content/workspace/data** along with the **TFRecod** files which we will create in the next step.\n",
        "\n",
        "An example label map `(e.g label_map.pbtxt)`, assuming that our dataset containes 2 labels, dogs and cats looks like this :\n",
        "\n",
        "```\n",
        "item {\n",
        "    id: 1\n",
        "    name: 'cat'\n",
        "}\n",
        "\n",
        "item {\n",
        "    id: 2\n",
        "    name: 'dog'\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYOn6MZ2gWTV"
      },
      "source": [
        "#Create a label_map.pbtxt file : This label map is used both by the training and detection processes.\n",
        "unique_labels  = list(train_data.labels.unique())\n",
        "integer_labels = le.transform(unique_labels) + 1\n",
        "\n",
        "label_dict = {unique_labels[i] : integer_labels[i] for i in range(len(unique_labels))}\n",
        "\n",
        "label_map = \"/content/workspace/data/label_map.pbtxt\"\n",
        "categories = train_data.labels.unique()\n",
        "categories.sort()\t\n",
        "\n",
        "end = '\\n'\n",
        "s = ' '\n",
        "\n",
        "for name in categories:\n",
        "    out = ''\n",
        "    out += 'item' + s + '{' + end\n",
        "    out += s*2 + 'id:' + ' ' + (str(label_dict[name])) + end\n",
        "    out += s*2 + 'name:' + ' ' + '\\'' + name + '\\'' + end\n",
        "    out += '}' + end*2\n",
        "    \n",
        "    with open(label_map, 'a') as f:\n",
        "        f.write(out)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-2WZiy_rUxh"
      },
      "source": [
        "Our **`label_map.pbtxt`** file will look like this :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vat0S2CNN9v4",
        "outputId": "26333fe9-11fb-4ba4-a1fa-240ea38b2b82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#hide_input\n",
        "!cat \"/content/workspace/data/label_map.pbtxt\""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "item {\n",
            "  id: 1\n",
            "  name: 'abyssinian'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 2\n",
            "  name: 'american_bulldog'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 3\n",
            "  name: 'american_pit_bull_terrier'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 4\n",
            "  name: 'basset_hound'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 5\n",
            "  name: 'beagle'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 6\n",
            "  name: 'bengal'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 7\n",
            "  name: 'birman'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 8\n",
            "  name: 'bombay'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 9\n",
            "  name: 'boxer'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 10\n",
            "  name: 'british_shorthair'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 11\n",
            "  name: 'chihuahua'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 12\n",
            "  name: 'egyptian_mau'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 13\n",
            "  name: 'english_cocker_spaniel'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 14\n",
            "  name: 'english_setter'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 15\n",
            "  name: 'german_shorthaired'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 16\n",
            "  name: 'great_pyrenees'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 17\n",
            "  name: 'havanese'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 18\n",
            "  name: 'japanese_chin'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 19\n",
            "  name: 'keeshond'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 20\n",
            "  name: 'leonberger'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 21\n",
            "  name: 'maine_coon'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 22\n",
            "  name: 'miniature_pinscher'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 23\n",
            "  name: 'newfoundland'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 24\n",
            "  name: 'persian'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 25\n",
            "  name: 'pomeranian'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 26\n",
            "  name: 'pug'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 27\n",
            "  name: 'ragdoll'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 28\n",
            "  name: 'russian_blue'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 29\n",
            "  name: 'saint_bernard'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 30\n",
            "  name: 'samoyed'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 31\n",
            "  name: 'scottish_terrier'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 32\n",
            "  name: 'shiba_inu'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 33\n",
            "  name: 'siamese'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 34\n",
            "  name: 'sphynx'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 35\n",
            "  name: 'staffordshire_bull_terrier'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 36\n",
            "  name: 'wheaten_terrier'\n",
            "}\n",
            "\n",
            "item {\n",
            "  id: 37\n",
            "  name: 'yorkshire_terrier'\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U2E5iSCN-Cf"
      },
      "source": [
        "The **`label_map.pbtxt`** file has been placed under **/content/workspace/data/label_map.pbtxt**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPUXb9D00XtV"
      },
      "source": [
        "## 4. **Create TensorFlow Records** :\n",
        "\n",
        "In this step we will convert our annotatinos present in the pandas dataframe object into **TFRecord** format.\n",
        "\n",
        "For every example in our dataset, we should have the following information:\n",
        "- An RGB image for the dataset encoded as jpeg or png.\n",
        "- A bounding box coordinates for each image `(with origin in top left corner)` defined by 4 floating point numbers `[ymin, xmin, ymax, xmax]`.\n",
        "- The class of the object in the bounding box.\n",
        "\n",
        "\n",
        "> Note: For the bounding-boxes, the normalized coordinates (x / width, y / height) are stored in the **TFRecord** dataset. \n",
        "\n",
        "\n",
        "\n",
        "Since our dataset has more than a fairly large number of annotations we will shard your dataset into multiple files.\n",
        "Instead of writing all tf.Example protos to a single file we will store the dataset into multiple files .\n",
        "\n",
        "Our dataset is going to look something like this:\n",
        "\n",
        "```bash\n",
        "/{directory_path}/dataset.record-00000-00010\n",
        "/{directory_path}/dataset.record-00001-00010\n",
        "...\n",
        "/{directory_path}/dataset.record-00009-00010\n",
        "```\n",
        "\n",
        "Our train dataset is going to be stored as :\n",
        "\n",
        "```bash\n",
        "/content/workspace/data/train.record-00000-of-00010\n",
        "/content/workspace/data/train.record-00001-of-00010\n",
        "...\n",
        "/content/workspace/data/train.record-00009-of-00010\n",
        "```\n",
        "\n",
        "Similary for the test dataset :\n",
        "\n",
        "```bash\n",
        "/content/workspace/data/test.record-00000-of-00010\n",
        "/content/workspace/data/test.record-00001-of-00010\n",
        "...\n",
        "/content/workspace/data/test.record-00009-of-00010\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRbq450WraVj"
      },
      "source": [
        "def create_tf_example(fname, data):\n",
        "    \"\"\"\n",
        "    Creates a tf.Example proto from a single image\n",
        "    from the given data\n",
        "\n",
        "    Args:\n",
        "        fname: filename of a single image from data.\n",
        "        data : a pandas dataframe object in the format \n",
        "               specified in step 2. \n",
        "\n",
        "    Returns:\n",
        "        example: The created tf.Example.\n",
        "    \"\"\"\n",
        "    curr_data = data.loc[data.filename == fname]\n",
        "    \n",
        "    filename = fname.encode('utf8') # Filename of the image\n",
        "    height = curr_data[\"height\"].values[0] # Image height\n",
        "    width = curr_data[\"width\"].values[0] # Image width\n",
        "    \n",
        "    image_format = b'jpeg' # b'jpeg' or b'png'\n",
        "    \n",
        "    # List of normalized left x coordinates in bounding box (1 per box).\n",
        "    xmins = list(curr_data[\"xmin\"].values / width) \n",
        "    # List of normalized right x coordinates in bounding box (1 per box).\n",
        "    xmaxs = list(curr_data[\"xmax\"].values / width) \n",
        "    # List of normalized top y coordinates in bounding box (1 per box).\n",
        "    ymins = list(curr_data[\"ymin\"].values / height)\n",
        "    # List of normalized bottom y coordinates in bounding box (1 per box).\n",
        "    ymaxs = list(curr_data[\"ymax\"].values / height)\n",
        "    \n",
        "    # List of string class name of bounding box (1 per box)\n",
        "    classes_text = list(curr_data[\"labels\"].values)\n",
        "    classes_text = [text.encode('utf8') for text in classes_text]\n",
        "    \n",
        "    # List of integer class id of bounding box (1 per box)\n",
        "    classes = list(curr_data[\"encoded_label\"].values) \n",
        "\n",
        "    with tf1.gfile.GFile(filename, 'rb') as fid:\n",
        "        encoded_image_data = fid.read() # Encoded image bytes\n",
        "\n",
        "    features = tf1.train.Example(features=tf1.train.Features(feature={\n",
        "      'image/height': dataset_util.int64_feature(height),\n",
        "      'image/width': dataset_util.int64_feature(width),\n",
        "      'image/filename': dataset_util.bytes_feature(filename),\n",
        "      'image/source_id': dataset_util.bytes_feature(filename),\n",
        "      'image/encoded': dataset_util.bytes_feature(encoded_image_data),\n",
        "      'image/format': dataset_util.bytes_feature(image_format),\n",
        "      'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "      'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "      'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "      'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "      'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "      }))\n",
        "    \n",
        "    return features\n",
        "\n",
        "\n",
        "def create_records(output_path, data, shards=10):\n",
        "    \"\"\"\n",
        "    Fn iterates over all the annotations in dataset and creates a \n",
        "    sharded TFRecord dataset and additionally saves the sharded TFRecord dataset\n",
        "    to output path.\n",
        "\n",
        "    Args:\n",
        "        output_path: Path where to save the dataset\n",
        "        data       : A pandas Dataframe object as specified in step-2.\n",
        "        shards     : Number of the shards over which to save the dataset.\n",
        "                     The dataset is going to saved inside `shards` no. of files.\n",
        "    \"\"\"\n",
        "    writer = tf1.python_io.TFRecordWriter(output_path)\n",
        "    fnames = list(data.filename.unique())\n",
        "\n",
        "    with contextlib2.ExitStack() as tf_record_close_stack:\n",
        "        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(tf_record_close_stack,output_path,shards)\n",
        "        #enumerate over all the unique images present in the dataset\n",
        "        #and create a tf.Example proto for the particular annotations.\n",
        "        for index, fname in enumerate(tqdm(fnames)):\n",
        "            tf_example = create_tf_example(fname, data)\n",
        "            output_shard_index = index % shards\n",
        "            output_tfrecords[output_shard_index].write(tf_example.SerializeToString())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6jQCjuZyv1R"
      },
      "source": [
        "#Generate the TF Records\n",
        "create_records(\"/content/workspace/data/train.record\", data=train_data)\n",
        "create_records(\"/content/workspace/data/test.record\",  data=test_data )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS-y0jUN49o1"
      },
      "source": [
        "Our dataset is now prepared for training using a model from **[TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)** . \n",
        "\n",
        "The directory structure for the workspace should look something like this at this stage:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qLxMnF07U8_"
      },
      "source": [
        "#hide_input\n",
        "!tree -C \"/content/workspace\" --filelimit=100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJMZL6EL9pgm"
      },
      "source": [
        "# Configure Custom TensorFlow2 Object Detection Training Configuration\n",
        "\n",
        "> In this section we will download a pretrained-model from the [TF2 OD model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) and set up out training configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRCP0Jld97t1"
      },
      "source": [
        "In this tutorial we are going to implement the lightweight, smallest state of the art **efficientdet model**. \n",
        "\n",
        "\n",
        "We will create a directory call pretrained-models in our wokspace folder.\n",
        "\n",
        "We will download the latest pre-trained network for the model we wish to use. This can be in [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md).\n",
        "\n",
        "Once the ***.tar.gz** file has been downloaded, we will extract the file contents into. **/content/workspace/pre-trained-models** ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxZVSbqtzv_5"
      },
      "source": [
        "# Download the latest-pretrained weights for the efficientdet_d0 model and the config file\n",
        "\n",
        "#LINK : http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz\n",
        "model_name = \"efficientdet_d0_coco17_tpu-32\"\n",
        "model = \"efficientdet_d0_coco17_tpu-32.tar.gz\"\n",
        "\n",
        "os.makedirs(\"/content/workspace/pre_trained_models/\", exist_ok=True)\n",
        "download_tar = f\"http://download.tensorflow.org/models/object_detection/tf2/20200711/{model}\"\n",
        "!wget {download_tar} -P \"/content/workspace/pre_trained_models/\"\n",
        "\n",
        "tar = tarfile.open(f\"/content/workspace/pre_trained_models/{model}\")\n",
        "tar.extractall(path=\"/content/workspace/pre_trained_models/\")\n",
        "tar.close()\n",
        "\n",
        "os.unlink(f\"/content/workspace/pre_trained_models/{model}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y8eJm0L9nQB"
      },
      "source": [
        "The directory structure for the workspace should look something like this at this stage:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXXh6qwOKpVK"
      },
      "source": [
        "#hide_input\n",
        "!tree -C \"/content/workspace\" --filelimit=100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycUDtDdn_KNj"
      },
      "source": [
        "Now that we have downloaded and extracted our pre-trained model, let’s create a directory for our training job. Under the **/content/workspace/** create a new directory named **models** this will be the folder where  we will store all the configurations, model_checkpoints, logs for our custom trained model.\n",
        "\n",
        "Under the **/content/workspace/models/** dir create a dir named as **efficientdet_d0_coco17_tpu-32** and copy the **/content/workspace/pre-trained-models/efficientdet_d0_coco17_tpu-32/pipeline.config** file inside the newly created directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mx-x2HG7Fmh"
      },
      "source": [
        "os.makedirs(\"/content/workspace/models/\", exist_ok=True)\n",
        "os.makedirs(f\"/content/workspace/models/{model_name}\", exist_ok=True)\n",
        "\n",
        "config_path = f\"/content/workspace/pre_trained_models/{model_name}/pipeline.config\"\n",
        "shutil.copy2(config_path, f\"/content/workspace/models/{model_name}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWqCYtziAh7F"
      },
      "source": [
        "Each model has a model_name, a pipeline.config file, a pretrained_checkpoint. \n",
        "\n",
        "The pipeline.config file is a shell of a training configuration specific to each model type, provided by the authors of the TF2 OD repository. \n",
        "\n",
        "The pretrained_checkpoint is the location of a pretrained weights file saved from when the object detection model was pretrained on the COCO dataset. \n",
        "\n",
        "We will start from these weights, and then fine tune into our particular custom dataset task. By using pretraining, our model does not need to start from square one in identifying what features might be useful for object detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgqsAAGpACTt"
      },
      "source": [
        "We will map our training data files to variables for use in our computer vision training pipeline configuration. \n",
        "\n",
        "We will now edit the **/content/workspace/models/pipeline.config** to point to our custom data, the pretrained_checkpoint, and we also specify some training parameters. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt07xU0F9Dfp"
      },
      "source": [
        "#Path to train and test TFRecords\n",
        "test_record_fname = \"/content/workspace/data/test.record-?????-of-00010\"\n",
        "train_record_fname = \"/content/workspace/data/train.record-?????-of-00010\"\n",
        "\n",
        "#Path to the TensorFlow Object Detection format label_map\n",
        "label_map_pbtxt_fname = \"/content/workspace/data/label_map.pbtxt\"\n",
        "\n",
        "#Path to the pipeline.config file\n",
        "config_path = f\"/content/workspace/models/{model_name}/pipeline.config\"\n",
        "\n",
        "#Path to the pretrained model checkpoints \n",
        "fine_tune = f\"/content/workspace/pre_trained_models/{model_name}/checkpoint/ckpt-0\"\n",
        "\n",
        "#if you can fit a large batch in memory, it may speed up your training\n",
        "batch_size = 16\n",
        "#The more steps, the longer the training\n",
        "num_steps =  50000\n",
        "\n",
        "model_dir = f\"/content/workspace/models/{model_name}\"\n",
        "\n",
        "\n",
        "def get_num_classes(pbtxt_fname):\n",
        "    \"\"\"Get total number of classes from label_map.pbtxt file\"\"\"\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())\n",
        "\n",
        "\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "\n",
        "print(\"CUSTOM CONFIGURATION PARAMETERS : \")\n",
        "print(\"Config Path: \", config_path)\n",
        "print(\"Checkpoint Path: \", fine_tune)\n",
        "print(\"Label Map: \", label_map_pbtxt_fname)\n",
        "print(\"Train TFRecords: \", train_record_fname)\n",
        "print(\"Test TFRecords: \", test_record_fname)\n",
        "print(\"Total Steps: \", num_steps)\n",
        "print(\"Num classes: \", num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VaPrHNz9eJ5"
      },
      "source": [
        "#write custom configuration file by slotting our dataset, model checkpoint, and training parameters into the base pipeline file\n",
        "\n",
        "with open(config_path) as f:\n",
        "    s = f.read()\n",
        "\n",
        "with open(config_path, 'w') as f:\n",
        "    # fine_tune_checkpoint\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"', 'fine_tune_checkpoint: \"{}\"'.format(fine_tune), s)\n",
        "    # tfrecord files train and test\n",
        "    s = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    s = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n",
        "    # label_map_path\n",
        "    s = re.sub('label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "    # Set training batch_size.\n",
        "    s = re.sub('batch_size: [0-9]+','batch_size: {}'.format(batch_size), s)\n",
        "    # Set training steps, num_steps\n",
        "    s = re.sub('num_steps: [0-9]+', 'num_steps: {}'.format(num_steps), s)\n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('num_classes: [0-9]+','num_classes: {}'.format(num_classes), s)\n",
        "    #fine-tune checkpoint type\n",
        "    s = re.sub('fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n",
        "        \n",
        "    f.write(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tn3_bzeER8o"
      },
      "source": [
        "After slotting in out custom parameters into the **pipeline.config** file we will have a configuration file that is going to look something like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-a2hJfS_qEe"
      },
      "source": [
        "#hide_input\n",
        "!cat {config_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWA3pzVPErKV"
      },
      "source": [
        "# Train Custom TF2 Object Detector\n",
        "\n",
        "\n",
        "To initiate a new training job, we need to run the script **/content/models/research/object_detection/model_main_tf2.py** \n",
        "\n",
        "\n",
        "* config_path: path to the configuration file defined above in writing custom training configuration.\n",
        "\n",
        "* model_dir: the location tensorboard logs and saved model checkpoints will save to"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVPCaSGpBz_R"
      },
      "source": [
        "#hide_output\n",
        "\n",
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={config_path} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGGLw7DDFGnH"
      },
      "source": [
        "To evaluate our model on COCO-Evaluation metrics we need to run the script **/content/models/research/object_detection/model_main_tf2.py** .\n",
        "\n",
        "\n",
        "> Note: This process automatically evaluates the model on the latest checkpoints that the training job generates. So we can also run this script in the backgound as our model keeps training and as checkpoints are generated the script will automatically evaluate the model on the COCO-metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9R2gYK2CW6Y"
      },
      "source": [
        "#hide_output\n",
        "\n",
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={config_path} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --checkpoint_dir={model_dir} \\\n",
        "    --alsologtostderr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHgQon_QF1C6"
      },
      "source": [
        "**Monitor Training Job Progress using TensorBoard:**\n",
        "\n",
        "We can either use one of the 2 commands:\n",
        "\n",
        "\n",
        "To open in a terminal :\n",
        "```bash\n",
        "tensorboard --logdir \"/content/workspace/models/efficientdet_d0_coco17_tpu-32/\n",
        "```\n",
        "\n",
        "For a Jupyter-Environment:\n",
        "```python\n",
        "%tensorboard --logdir \"/content/workspace/models/efficientdet_d0_coco17_tpu-32/\"\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q91mdKBUHCNj"
      },
      "source": [
        "We will have logs that are going to look similar to this :\n",
        "\n",
        "![logs](https://www.dropbox.com/s/b3pvc7po59xcvmc/demo_logs.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy2dU3wacdox"
      },
      "source": [
        "#hide\n",
        "%tensorboard --logdir \"/content/workspace/models/efficientdet_d0_coco17_tpu-32/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywjzMFyJJlLB"
      },
      "source": [
        "#Exporting a Trained Inference Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DitMUiA9JuEk"
      },
      "source": [
        "Once your training job is complete, you need to extract the newly trained inference graph, which will be later used to perform the object detection. This can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HI084X_KWBi"
      },
      "source": [
        "#see where our model saved weights\n",
        "%ls \"/content/workspace/models/efficientdet_d0_coco17_tpu-32/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcMaaY3nJ7Jr"
      },
      "source": [
        "# make a directory to save the exported graph\n",
        "os.makedirs(\"/content/workspace/exported_models/efficientdet_d0\", exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpmRU3r7cjCB"
      },
      "source": [
        "#Once your training job is complete, you need to extract the newly trained inference graph, \n",
        "#which will be later used to perform the object detection\n",
        "\n",
        "#path to save the exporter inference graph\n",
        "output_directory = \"/content/workspace/exported_models/efficientdet_d0\"\n",
        "\n",
        "#path to trained model checkpoints\n",
        "checkpoint_dir = \"/content/workspace/models/efficientdet_d0_coco17_tpu-32/\"\n",
        "\n",
        "#run script to export model weights\n",
        "!python /content/models/research/object_detection/exporter_main_v2.py \\\n",
        "    --trained_checkpoint_dir {checkpoint_dir} \\\n",
        "    --output_directory {output_directory} \\\n",
        "    --pipeline_config_path {config_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KUf6m8WJrYL"
      },
      "source": [
        "Let's checkout our directory structure :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNamBtC9c4o5"
      },
      "source": [
        "#hide_input\n",
        "!tree -C \"/content/workspace\" --filelimit=100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suER8t1qKfMZ"
      },
      "source": [
        "In the next part we are going to use this exported graph to do inference on custom images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06eYsRhyKeHB"
      },
      "source": [
        "# Run Inference on Test Images with Custom TensorFlow2 Object Detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CnKdvvgMFSB"
      },
      "source": [
        "In this section we will run inference on images using our Custom TensorFlow2 Object Detector exported graph\n",
        "\n",
        "\n",
        "To run inference we will create a few helper functions first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIHwthwxdzHX"
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: the file path to the image\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "  image = Image.open(BytesIO(img_data))\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "def load_model(model_dir):\n",
        "    model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
        "    model = tf.saved_model.load(str(model_dir))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEJm7IBmMWQ2"
      },
      "source": [
        "Load in the **category_index**, which is a dictionary mapping of the classes and the index labels & the **Custom TensorFlow2 Object Detector** from the exported graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQlDZDbqea3e"
      },
      "source": [
        "#path to the label map\n",
        "PATH_TO_LABELS = \"/content/workspace/data/label_map.pbtxt\"\n",
        "#generate a category index dictionary from the label map\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
        "\n",
        "#path to the images from the test directory we will reun inference on these images\n",
        "PATH_TO_TEST_IMAGES_DIR = pathlib.Path(\"/content/workspace/images/test/\")\n",
        "TEST_IMAGE_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob(\"*.jpg\")))\n",
        "\n",
        "\n",
        "#Load model from the exported model graph\n",
        "print('Loading model...', end='')\n",
        "start_time = time.time()\n",
        "detection_model = load_model(\"/content/workspace/exported_models/efficientdet_d0\")\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print('Done! Took {} seconds'.format(elapsed_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8Qh4npjfJiV"
      },
      "source": [
        "def run_inference_for_single_image(model, image):\n",
        "    \"\"\"\n",
        "    Fn to run inference on a single image\n",
        "    \"\"\"\n",
        "    image = np.asarray(image)\n",
        "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "    input_tensor = tf.convert_to_tensor(image)\n",
        "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "    input_tensor = input_tensor[tf.newaxis,...]\n",
        "    # Run inference\n",
        "    model_fn = model.signatures['serving_default']\n",
        "    output_dict = model_fn(input_tensor)\n",
        "    \n",
        "    # All outputs are batches tensors.\n",
        "    # Convert to numpy arrays, and take index [0] to remove the batch dimension\n",
        "    # We're only interested in the first num_detections.\n",
        "    num_detections = int(output_dict.pop('num_detections'))\n",
        "    output_dict = {key:value[0, :num_detections].numpy() for key,value in output_dict.items()}\n",
        "    output_dict['num_detections'] = num_detections\n",
        "    # detection_classes should be ints.\n",
        "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
        "    \n",
        "    # Handle models with masks\n",
        "    if 'detection_masks' in output_dict:\n",
        "        # Reframe the the bbox mask to the image size.\n",
        "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
        "            image.shape[0], \n",
        "            image.shape[1]\n",
        "            )      \n",
        "        \n",
        "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
        "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
        "    \n",
        "    return output_dict\n",
        "\n",
        "\n",
        "def show_inference(model, image_path, threshold = 0.5):\n",
        "    \"\"\"\n",
        "    Runs infernce on the given image at the image_path and also\n",
        "    draws the bounding box over the image .\n",
        "    \"\"\"\n",
        "    # the array based representation of the image will be used later in order to prepare the\n",
        "    # result image with boxes and labels on it.\n",
        "    print('Running inference for {}... '.format(image_path), end='')\n",
        "    image_np = np.array(Image.open(image_path))\n",
        "    # Actual detection.\n",
        "    output_dict = run_inference_for_single_image(model, image_np)\n",
        "    # Visualization of the results of a detection.\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np,\n",
        "        output_dict['detection_boxes'],\n",
        "        output_dict['detection_classes'],\n",
        "        output_dict['detection_scores'],\n",
        "        category_index,\n",
        "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
        "        use_normalized_coordinates=True,\n",
        "        max_boxes_to_draw=100,\n",
        "        min_score_thresh=threshold,\n",
        "        )\n",
        "    \n",
        "    print('Done !')\n",
        "    display(Image.fromarray(image_np))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioaovdftMn4o"
      },
      "source": [
        "Using the helper functions defined above let's run inference on Images :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dODEVBgPiUn4"
      },
      "source": [
        "#Get a random image from the test dataset\n",
        "path = TEST_IMAGE_PATHS[np.random.randint(0, len(TEST_IMAGE_PATHS))]\n",
        "#Run inference over the image and display the results\n",
        "show_inference(detection_model, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dwIyVfOiulx"
      },
      "source": [
        "#Get a random image from the test dataset\n",
        "path = TEST_IMAGE_PATHS[np.random.randint(0, len(TEST_IMAGE_PATHS))]\n",
        "#Run inference over the image and display the results\n",
        "show_inference(detection_model, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac1uuQSJjG34"
      },
      "source": [
        "#Get a random image from the test dataset\n",
        "path = TEST_IMAGE_PATHS[np.random.randint(0, len(TEST_IMAGE_PATHS))]\n",
        "#Run inference over the image and display the results\n",
        "show_inference(detection_model, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUJ-yeONRBt3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}