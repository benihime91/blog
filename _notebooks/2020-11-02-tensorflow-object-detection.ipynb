{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "2020-11-02-tensorflow-object-detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNirckkNBaRwvFI8Surio0i"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlHHd8rX8CLt"
      },
      "source": [
        "# TensorFlow Object Detection API Tutorial\n",
        "> TensorFlow recently announced [TF Object Detection API models to be TensorFlow 2 compatible](https://blog.tensorflow.org/2020/07/tensorflow-2-meets-object-detection-api.html) . In this tutorial we will go over on how to train a object detection model on custom dataset using TensorFlow Object Detection API 2.\n",
        "\n",
        "- toc: false\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [machinelearning deeplearning tensorflow2.x tensorflow-object-detection]\n",
        "- image: images/object-detection.jpg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwfw_ORfE1nu",
        "outputId": "6a9ec76d-2551-4a37-e8c2-1a49e61cea22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#hide\n",
        "!nvidia-smi\n",
        "\n",
        "#fname: _notebooks/2020-11-02-tensorflow-object-detection.ipynb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Nov  3 11:23:15 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALR2laa9PYpa"
      },
      "source": [
        "> Important: This guide is based on [official tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html) and could intersect with this [Tutorial from Roboflow-Team]((https://blog.roboflow.com/train-a-tensorflow2-object-detection-model/). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCCuhf1kmv3Q"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "In this notebook, we implement [The TensorFlow 2 Object Detection Library](https://blog.tensorflow.org/2020/07/tensorflow-2-meets-object-detection-api.html) for training on your own dataset.\n",
        "\n",
        "\n",
        "We will take the following steps to implement a model from **[TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)** on our custom data:\n",
        "* Install TensorFlow2 Object Detection Dependencies\n",
        "* Download Custom TensorFlow2 Object Detection Dataset\n",
        "* Write Custom TensorFlow2 Object Detection Training Configuation\n",
        "* Train Custom TensorFlow2 Object Detection Model\n",
        "* Export Custom TensorFlow2 Object Detection Weights\n",
        "* Use Trained TensorFlow2 Object Detection For Inference on Test Images\n",
        "\n",
        "When you are done you will have a custom detector that you can use. It will make inference like this:\n",
        "\n",
        "![](https://www.dropbox.com/s/slkbti9incj3k09/object-detection.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xEeVSgmoL6U"
      },
      "source": [
        "#Install TensorFlow2 Object Detection Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0X5ZEE1OyiR"
      },
      "source": [
        "To install **TensorFlow2 Object Detection** on Google-Colab run the following steps. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfNqKMd7YXAe",
        "outputId": "1ddf4d3a-e661-43bb-ce6d-023e6434064f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#hide_output\n",
        "import os\n",
        "import pathlib\n",
        "\n",
        "# Clone the tensorflow models repository if it doesn't already exist\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 2281, done.\u001b[K\n",
            "remote: Counting objects: 100% (2281/2281), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1977/1977), done.\u001b[K\n",
            "remote: Total 2281 (delta 556), reused 955 (delta 278), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2281/2281), 30.55 MiB | 27.35 MiB/s, done.\n",
            "Resolving deltas: 100% (556/556), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKS6DbhpYgo4",
        "outputId": "fd11c5c0-4167-4b2b-989a-852f39e1619d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#hide_output\n",
        "\n",
        "# Install the Object Detection API\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /content/models/research\n",
            "Collecting avro-python3\n",
            "  Downloading https://files.pythonhosted.org/packages/b2/5a/819537be46d65a01f8b8c6046ed05603fb9ef88c663b8cca840263788d58/avro-python3-1.10.0.tar.gz\n",
            "Collecting apache-beam\n",
            "  Downloading https://files.pythonhosted.org/packages/86/3f/93816e989e8e59b337f22927778494a99b2a3e78a3b6a9e34d043c6fab4e/apache_beam-2.25.0-cp36-cp36m-manylinux2010_x86_64.whl (8.7MB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (7.0.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (4.2.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (0.29.21)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (0.5.5)\n",
            "Collecting tf-slim\n",
            "  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (2.0.2)\n",
            "Collecting lvis\n",
            "  Downloading https://files.pythonhosted.org/packages/72/b6/1992240ab48310b5360bfdd1d53163f43bb97d90dc5dc723c67d41c38e78/lvis-0.5.3-py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (1.1.3)\n",
            "Collecting tf-models-official\n",
            "  Downloading https://files.pythonhosted.org/packages/5b/33/91e5e90e3e96292717245d3fe87eb3b35b07c8a2113f2da7f482040facdb/tf_models_official-2.3.0-py2.py3-none-any.whl (840kB)\n",
            "Requirement already satisfied: numpy<2,>=1.14.3 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (1.18.5)\n",
            "Requirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (4.1.3)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (2018.9)\n",
            "Collecting fastavro<2,>=0.21.4\n",
            "  Downloading https://files.pythonhosted.org/packages/af/54/1054887181f600414beba751290fe13dfa44214e91397aa0c867adb7d74b/fastavro-1.1.0-cp36-cp36m-manylinux2014_x86_64.whl (2.0MB)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/82/39/2c0879b1bcfd1f6ad078eb210d09dbce21072386a3997074ee91e60ddc5a/hdfs-2.5.8.tar.gz (41kB)\n",
            "Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (1.33.1)\n",
            "Requirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (0.17.4)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (3.11.0)\n",
            "Collecting pyarrow<0.18.0,>=0.15.1; python_version >= \"3.0\" or platform_system != \"Windows\"\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/3f/6cac1714fff444664603f92cb9fbe91c7ae25375880158b9e9691c4584c8/pyarrow-0.17.1-cp36-cp36m-manylinux2014_x86_64.whl (63.8MB)\n",
            "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (3.7.4.3)\n",
            "Collecting future<1.0.0,>=0.18.2\n",
            "  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "Collecting requests<3.0.0,>=2.24.0\n",
            "  Downloading https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl (61kB)\n",
            "Collecting mock<3.0.0,>=1.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)\n",
            "Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (3.12.4)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam->object-detection==0.1) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->object-detection==0.1) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->object-detection==0.1) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->object-detection==0.1) (0.10.0)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf-slim->object-detection==0.1) (0.10.0)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools->object-detection==0.1) (50.3.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.6/dist-packages (from lvis->object-detection==0.1) (4.1.2.30)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (0.3.0)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (0.8.3)\n",
            "Collecting opencv-python-headless\n",
            "  Downloading https://files.pythonhosted.org/packages/08/e9/57d869561389884136be65a2d1bc038fe50171e2ba348fda269a4aab8032/opencv_python_headless-4.4.0.46-cp36-cp36m-manylinux2014_x86_64.whl (36.7MB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (3.13)\n",
            "Collecting tensorflow-model-optimization>=0.2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/55/38/4fd48ea1bfcb0b6e36d949025200426fe9c3a8bfae029f0973d85518fa5a/tensorflow_model_optimization-0.5.0-py2.py3-none-any.whl (172kB)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (4.0.1)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (5.4.8)\n",
            "Requirement already satisfied: tensorflow>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (2.3.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (1.5.9)\n",
            "Collecting sentencepiece\n",
            "  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (0.9.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (0.7)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/f5/8e6e85ce2e9f6e05040cf0d4e26f43a4718bcc4bce988b433276d4b1a5c1/py-cpuinfo-7.0.0.tar.gz (95kB)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (1.21.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.6/dist-packages (from tf-models-official->object-detection==0.1) (1.7.12)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client<5,>=2.0.1->apache-beam->object-detection==0.1) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client<5,>=2.0.1->apache-beam->object-detection==0.1) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client<5,>=2.0.1->apache-beam->object-detection==0.1) (4.6)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (1.24.3)\n",
            "Collecting pbr>=0.11\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->tf-models-official->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization>=0.2.1->tf-models-official->object-detection==0.1) (0.1.5)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (0.24.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (4.41.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (20.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (0.35.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.1.2)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (2.3.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official->object-detection==0.1) (4.0.1)\n",
            "Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-official->object-detection==0.1) (0.0.1)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (0.4.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (1.0.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (0.0.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (1.17.2)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->tf-models-official->object-detection==0.1) (3.3.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->tf-models-official->object-detection==0.1) (1.52.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (3.3.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (0.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (1.16.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (2.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->tf-models-official->object-detection==0.1) (3.1.0)\n",
            "Building wheels for collected packages: object-detection, avro-python3, hdfs, future, dill, py-cpuinfo\n",
            "  Building wheel for object-detection (setup.py): started\n",
            "  Building wheel for object-detection (setup.py): finished with status 'done'\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-cp36-none-any.whl size=1597507 sha256=b61ea6ef096d6bc6f90647c7481a3dd89676573712761446b1a0e885771c8a4c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h5bnh8ei/wheels/94/49/4b/39b051683087a22ef7e80ec52152a27249d1a644ccf4e442ea\n",
            "  Building wheel for avro-python3 (setup.py): started\n",
            "  Building wheel for avro-python3 (setup.py): finished with status 'done'\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.10.0-cp36-none-any.whl size=43735 sha256=dbef078a2cc755e705d58c6237663ccd6803194e5e9e8e4f6bcb6d841a990939\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/15/cd/fe4ec8b88c130393464703ee8111e2cddebdc40e1b59ea85e9\n",
            "  Building wheel for hdfs (setup.py): started\n",
            "  Building wheel for hdfs (setup.py): finished with status 'done'\n",
            "  Created wheel for hdfs: filename=hdfs-2.5.8-cp36-none-any.whl size=33213 sha256=c896b37b0f225909892bf44a387a20a063d3080e07d1d17c337b98a561d578f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/a7/05/23e3699975fc20f8a30e00ac1e515ab8c61168e982abe4ce70\n",
            "  Building wheel for future (setup.py): started\n",
            "  Building wheel for future (setup.py): finished with status 'done'\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=fbc4616d73f5b8fbcbbed78f4cb42ee22906caef7869d416ee276fb8c5e7c791\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for dill (setup.py): started\n",
            "  Building wheel for dill (setup.py): finished with status 'done'\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-cp36-none-any.whl size=78532 sha256=1da59eaa3506b7276de87174769c930fb5fd476f2b931578990a0d57d50f7c63\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/b1/91/f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\n",
            "  Building wheel for py-cpuinfo (setup.py): started\n",
            "  Building wheel for py-cpuinfo (setup.py): finished with status 'done'\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-7.0.0-cp36-none-any.whl size=20071 sha256=abfa24e6dbf790256aa3daf04f8f8571f5dd8b70ccf69c601f394fa85ee0ae98\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/93/7b/127daf0c3a5a49feb2fecd468d508067c733fba5192f726ad1\n",
            "Successfully built object-detection avro-python3 hdfs future dill py-cpuinfo\n",
            "Installing collected packages: avro-python3, fastavro, requests, hdfs, pyarrow, future, pbr, mock, dill, apache-beam, tf-slim, lvis, opencv-python-headless, tensorflow-model-optimization, sentencepiece, py-cpuinfo, tf-models-official, object-detection\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: dill 0.3.2\n",
            "    Uninstalling dill-0.3.2:\n",
            "      Successfully uninstalled dill-0.3.2\n",
            "Successfully installed apache-beam-2.25.0 avro-python3-1.10.0 dill-0.3.1.1 fastavro-1.1.0 future-0.18.2 hdfs-2.5.8 lvis-0.5.3 mock-2.0.0 object-detection-0.1 opencv-python-headless-4.4.0.46 pbr-5.5.1 py-cpuinfo-7.0.0 pyarrow-0.17.1 requests-2.24.0 sentencepiece-0.1.94 tensorflow-model-optimization-0.5.0 tf-models-official-2.3.0 tf-slim-1.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR: multiprocess 0.70.10 has requirement dill>=0.3.2, but you'll have dill 0.3.1.1 which is incompatible.\n",
            "ERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.24.0 which is incompatible.\n",
            "ERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\n",
            "ERROR: apache-beam 2.25.0 has requirement avro-python3!=1.9.2,<1.10.0,>=1.8.1; python_version >= \"3.0\", but you'll have avro-python3 1.10.0 which is incompatible.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK4R-kbYo2vK"
      },
      "source": [
        "Run the TF2 model builder tests to make sure our environment is up and running. If successful  If successful, you should see the following outputs at the end of the cell execution printouts.\n",
        "\n",
        "```\n",
        "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
        "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
        "----------------------------------------------------------------------\n",
        "Ran 20 tests in 52.705s\n",
        "\n",
        "OK (skipped=1)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36KcetmMcEzh",
        "outputId": "83e9137c-54f1-47af-c992-1ca16a6761c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#hide_output\n",
        "\n",
        "#run model builder test to \n",
        "!python /content/models/research/object_detection/builders/model_builder_tf2_test.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-03 11:24:34.537771: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Running tests under Python 3.6.9: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model\n",
            "2020-11-03 11:24:36.669000: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-11-03 11:24:36.719922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-03 11:24:36.720507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-11-03 11:24:36.720588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-03 11:24:36.944809: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-03 11:24:37.062981: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-03 11:24:37.079070: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-03 11:24:37.365502: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-03 11:24:37.386861: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-03 11:24:37.941773: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-03 11:24:37.941965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-03 11:24:37.942592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-03 11:24:37.943120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-03 11:24:37.962687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz\n",
            "2020-11-03 11:24:37.962901: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14c1480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-03 11:24:37.962927: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-11-03 11:24:38.095521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-03 11:24:38.096203: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14c1640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-03 11:24:38.096233: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-11-03 11:24:38.097335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-03 11:24:38.097876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-11-03 11:24:38.097921: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-03 11:24:38.097968: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-03 11:24:38.097989: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-03 11:24:38.098010: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-03 11:24:38.098033: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-03 11:24:38.098051: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-03 11:24:38.098072: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-03 11:24:38.098146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-03 11:24:38.098679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-03 11:24:38.099179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-03 11:24:38.101357: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-03 11:24:41.864903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-11-03 11:24:41.864947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-11-03 11:24:41.864966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-11-03 11:24:41.868085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-03 11:24:41.868805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-03 11:24:41.869364: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-11-03 11:24:41.869411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13962 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model): 9.62s\n",
            "I1103 11:24:46.043210 140273826584448 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_center_net_model): 9.62s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "I1103 11:24:46.044465 140273826584448 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.03s\n",
            "I1103 11:24:46.076314 140273826584448 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.03s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.02s\n",
            "I1103 11:24:46.096552 140273826584448 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.02s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.02s\n",
            "I1103 11:24:46.117159 140273826584448 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.02s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.14s\n",
            "I1103 11:24:46.253935 140273826584448 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.14s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.14s\n",
            "I1103 11:24:46.391033 140273826584448 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.14s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.14s\n",
            "I1103 11:24:46.533700 140273826584448 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.14s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.14s\n",
            "I1103 11:24:46.670073 140273826584448 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.14s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.15s\n",
            "I1103 11:24:46.818175 140273826584448 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.15s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.04s\n",
            "I1103 11:24:46.859567 140273826584448 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.04s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "I1103 11:24:47.145989 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
            "I1103 11:24:47.146132 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 64\n",
            "I1103 11:24:47.146209 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 3\n",
            "I1103 11:24:47.151921 140273826584448 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1103 11:24:47.174776 140273826584448 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1103 11:24:47.174907 140273826584448 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1103 11:24:47.265388 140273826584448 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1103 11:24:47.265522 140273826584448 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1103 11:24:47.470117 140273826584448 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1103 11:24:47.470269 140273826584448 efficientnet_model.py:148] round_filter input=40 output=40\n",
            "I1103 11:24:47.674718 140273826584448 efficientnet_model.py:148] round_filter input=40 output=40\n",
            "I1103 11:24:47.674886 140273826584448 efficientnet_model.py:148] round_filter input=80 output=80\n",
            "I1103 11:24:47.982617 140273826584448 efficientnet_model.py:148] round_filter input=80 output=80\n",
            "I1103 11:24:47.982774 140273826584448 efficientnet_model.py:148] round_filter input=112 output=112\n",
            "I1103 11:24:48.278846 140273826584448 efficientnet_model.py:148] round_filter input=112 output=112\n",
            "I1103 11:24:48.278996 140273826584448 efficientnet_model.py:148] round_filter input=192 output=192\n",
            "I1103 11:24:48.787364 140273826584448 efficientnet_model.py:148] round_filter input=192 output=192\n",
            "I1103 11:24:48.787523 140273826584448 efficientnet_model.py:148] round_filter input=320 output=320\n",
            "I1103 11:24:48.873517 140273826584448 efficientnet_model.py:148] round_filter input=1280 output=1280\n",
            "I1103 11:24:48.910067 140273826584448 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1103 11:24:48.988487 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
            "I1103 11:24:48.988615 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 88\n",
            "I1103 11:24:48.988685 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 4\n",
            "I1103 11:24:48.993713 140273826584448 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1103 11:24:49.019097 140273826584448 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1103 11:24:49.019210 140273826584448 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1103 11:24:49.158173 140273826584448 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1103 11:24:49.158296 140273826584448 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1103 11:24:49.466279 140273826584448 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1103 11:24:49.466430 140273826584448 efficientnet_model.py:148] round_filter input=40 output=40\n",
            "I1103 11:24:49.772570 140273826584448 efficientnet_model.py:148] round_filter input=40 output=40\n",
            "I1103 11:24:49.772728 140273826584448 efficientnet_model.py:148] round_filter input=80 output=80\n",
            "I1103 11:24:50.155907 140273826584448 efficientnet_model.py:148] round_filter input=80 output=80\n",
            "I1103 11:24:50.156064 140273826584448 efficientnet_model.py:148] round_filter input=112 output=112\n",
            "I1103 11:24:50.527935 140273826584448 efficientnet_model.py:148] round_filter input=112 output=112\n",
            "I1103 11:24:50.528086 140273826584448 efficientnet_model.py:148] round_filter input=192 output=192\n",
            "I1103 11:24:51.010271 140273826584448 efficientnet_model.py:148] round_filter input=192 output=192\n",
            "I1103 11:24:51.010432 140273826584448 efficientnet_model.py:148] round_filter input=320 output=320\n",
            "I1103 11:24:51.194444 140273826584448 efficientnet_model.py:148] round_filter input=1280 output=1280\n",
            "I1103 11:24:51.231679 140273826584448 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1103 11:24:51.321885 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
            "I1103 11:24:51.322017 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 112\n",
            "I1103 11:24:51.322098 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 5\n",
            "I1103 11:24:51.327182 140273826584448 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1103 11:24:51.348463 140273826584448 efficientnet_model.py:148] round_filter input=32 output=32\n",
            "I1103 11:24:51.348572 140273826584448 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1103 11:24:51.651104 140273826584448 efficientnet_model.py:148] round_filter input=16 output=16\n",
            "I1103 11:24:51.651269 140273826584448 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1103 11:24:51.935949 140273826584448 efficientnet_model.py:148] round_filter input=24 output=24\n",
            "I1103 11:24:51.936112 140273826584448 efficientnet_model.py:148] round_filter input=40 output=48\n",
            "I1103 11:24:52.217791 140273826584448 efficientnet_model.py:148] round_filter input=40 output=48\n",
            "I1103 11:24:52.218012 140273826584448 efficientnet_model.py:148] round_filter input=80 output=88\n",
            "I1103 11:24:52.632406 140273826584448 efficientnet_model.py:148] round_filter input=80 output=88\n",
            "I1103 11:24:52.632575 140273826584448 efficientnet_model.py:148] round_filter input=112 output=120\n",
            "I1103 11:24:53.051553 140273826584448 efficientnet_model.py:148] round_filter input=112 output=120\n",
            "I1103 11:24:53.051720 140273826584448 efficientnet_model.py:148] round_filter input=192 output=208\n",
            "I1103 11:24:53.536417 140273826584448 efficientnet_model.py:148] round_filter input=192 output=208\n",
            "I1103 11:24:53.536584 140273826584448 efficientnet_model.py:148] round_filter input=320 output=352\n",
            "I1103 11:24:53.726126 140273826584448 efficientnet_model.py:148] round_filter input=1280 output=1408\n",
            "I1103 11:24:53.760966 140273826584448 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1103 11:24:53.852025 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
            "I1103 11:24:53.852160 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 160\n",
            "I1103 11:24:53.852231 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 6\n",
            "I1103 11:24:53.857233 140273826584448 efficientnet_model.py:148] round_filter input=32 output=40\n",
            "I1103 11:24:53.878922 140273826584448 efficientnet_model.py:148] round_filter input=32 output=40\n",
            "I1103 11:24:53.879034 140273826584448 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1103 11:24:54.017628 140273826584448 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1103 11:24:54.017755 140273826584448 efficientnet_model.py:148] round_filter input=24 output=32\n",
            "I1103 11:24:54.301676 140273826584448 efficientnet_model.py:148] round_filter input=24 output=32\n",
            "I1103 11:24:54.301889 140273826584448 efficientnet_model.py:148] round_filter input=40 output=48\n",
            "I1103 11:24:54.581738 140273826584448 efficientnet_model.py:148] round_filter input=40 output=48\n",
            "I1103 11:24:54.581910 140273826584448 efficientnet_model.py:148] round_filter input=80 output=96\n",
            "I1103 11:24:55.053458 140273826584448 efficientnet_model.py:148] round_filter input=80 output=96\n",
            "I1103 11:24:55.053621 140273826584448 efficientnet_model.py:148] round_filter input=112 output=136\n",
            "I1103 11:24:55.724524 140273826584448 efficientnet_model.py:148] round_filter input=112 output=136\n",
            "I1103 11:24:55.724686 140273826584448 efficientnet_model.py:148] round_filter input=192 output=232\n",
            "I1103 11:24:56.317873 140273826584448 efficientnet_model.py:148] round_filter input=192 output=232\n",
            "I1103 11:24:56.318033 140273826584448 efficientnet_model.py:148] round_filter input=320 output=384\n",
            "I1103 11:24:56.504783 140273826584448 efficientnet_model.py:148] round_filter input=1280 output=1536\n",
            "I1103 11:24:56.544883 140273826584448 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1103 11:24:56.647302 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
            "I1103 11:24:56.647446 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 224\n",
            "I1103 11:24:56.647518 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 7\n",
            "I1103 11:24:56.652609 140273826584448 efficientnet_model.py:148] round_filter input=32 output=48\n",
            "I1103 11:24:56.673902 140273826584448 efficientnet_model.py:148] round_filter input=32 output=48\n",
            "I1103 11:24:56.674020 140273826584448 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1103 11:24:56.814806 140273826584448 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1103 11:24:56.814946 140273826584448 efficientnet_model.py:148] round_filter input=24 output=32\n",
            "I1103 11:24:57.200655 140273826584448 efficientnet_model.py:148] round_filter input=24 output=32\n",
            "I1103 11:24:57.200804 140273826584448 efficientnet_model.py:148] round_filter input=40 output=56\n",
            "I1103 11:24:57.622812 140273826584448 efficientnet_model.py:148] round_filter input=40 output=56\n",
            "I1103 11:24:57.622994 140273826584448 efficientnet_model.py:148] round_filter input=80 output=112\n",
            "I1103 11:24:58.207941 140273826584448 efficientnet_model.py:148] round_filter input=80 output=112\n",
            "I1103 11:24:58.208102 140273826584448 efficientnet_model.py:148] round_filter input=112 output=160\n",
            "I1103 11:24:58.820248 140273826584448 efficientnet_model.py:148] round_filter input=112 output=160\n",
            "I1103 11:24:58.820421 140273826584448 efficientnet_model.py:148] round_filter input=192 output=272\n",
            "I1103 11:24:59.654454 140273826584448 efficientnet_model.py:148] round_filter input=192 output=272\n",
            "I1103 11:24:59.654661 140273826584448 efficientnet_model.py:148] round_filter input=320 output=448\n",
            "I1103 11:24:59.850536 140273826584448 efficientnet_model.py:148] round_filter input=1280 output=1792\n",
            "I1103 11:24:59.885361 140273826584448 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1103 11:24:59.997732 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
            "I1103 11:24:59.997897 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 288\n",
            "I1103 11:24:59.997976 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 7\n",
            "I1103 11:25:00.003246 140273826584448 efficientnet_model.py:148] round_filter input=32 output=48\n",
            "I1103 11:25:00.024959 140273826584448 efficientnet_model.py:148] round_filter input=32 output=48\n",
            "I1103 11:25:00.025068 140273826584448 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1103 11:25:00.478423 140273826584448 efficientnet_model.py:148] round_filter input=16 output=24\n",
            "I1103 11:25:00.478590 140273826584448 efficientnet_model.py:148] round_filter input=24 output=40\n",
            "I1103 11:25:00.956959 140273826584448 efficientnet_model.py:148] round_filter input=24 output=40\n",
            "I1103 11:25:00.957116 140273826584448 efficientnet_model.py:148] round_filter input=40 output=64\n",
            "I1103 11:25:01.442008 140273826584448 efficientnet_model.py:148] round_filter input=40 output=64\n",
            "I1103 11:25:01.442172 140273826584448 efficientnet_model.py:148] round_filter input=80 output=128\n",
            "I1103 11:25:02.152642 140273826584448 efficientnet_model.py:148] round_filter input=80 output=128\n",
            "I1103 11:25:02.152853 140273826584448 efficientnet_model.py:148] round_filter input=112 output=176\n",
            "I1103 11:25:02.822890 140273826584448 efficientnet_model.py:148] round_filter input=112 output=176\n",
            "I1103 11:25:02.823047 140273826584448 efficientnet_model.py:148] round_filter input=192 output=304\n",
            "I1103 11:25:03.677937 140273826584448 efficientnet_model.py:148] round_filter input=192 output=304\n",
            "I1103 11:25:03.678102 140273826584448 efficientnet_model.py:148] round_filter input=320 output=512\n",
            "I1103 11:25:03.973348 140273826584448 efficientnet_model.py:148] round_filter input=1280 output=2048\n",
            "I1103 11:25:04.009898 140273826584448 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1103 11:25:04.130071 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
            "I1103 11:25:04.130211 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 384\n",
            "I1103 11:25:04.130283 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 8\n",
            "I1103 11:25:04.135291 140273826584448 efficientnet_model.py:148] round_filter input=32 output=56\n",
            "I1103 11:25:04.156319 140273826584448 efficientnet_model.py:148] round_filter input=32 output=56\n",
            "I1103 11:25:04.156429 140273826584448 efficientnet_model.py:148] round_filter input=16 output=32\n",
            "I1103 11:25:04.386298 140273826584448 efficientnet_model.py:148] round_filter input=16 output=32\n",
            "I1103 11:25:04.386444 140273826584448 efficientnet_model.py:148] round_filter input=24 output=40\n",
            "I1103 11:25:04.994280 140273826584448 efficientnet_model.py:148] round_filter input=24 output=40\n",
            "I1103 11:25:04.994444 140273826584448 efficientnet_model.py:148] round_filter input=40 output=72\n",
            "I1103 11:25:05.587292 140273826584448 efficientnet_model.py:148] round_filter input=40 output=72\n",
            "I1103 11:25:05.587471 140273826584448 efficientnet_model.py:148] round_filter input=80 output=144\n",
            "I1103 11:25:06.646055 140273826584448 efficientnet_model.py:148] round_filter input=80 output=144\n",
            "I1103 11:25:06.646228 140273826584448 efficientnet_model.py:148] round_filter input=112 output=200\n",
            "I1103 11:25:07.421711 140273826584448 efficientnet_model.py:148] round_filter input=112 output=200\n",
            "I1103 11:25:07.421891 140273826584448 efficientnet_model.py:148] round_filter input=192 output=344\n",
            "I1103 11:25:08.531391 140273826584448 efficientnet_model.py:148] round_filter input=192 output=344\n",
            "I1103 11:25:08.531574 140273826584448 efficientnet_model.py:148] round_filter input=320 output=576\n",
            "I1103 11:25:08.835965 140273826584448 efficientnet_model.py:148] round_filter input=1280 output=2304\n",
            "I1103 11:25:08.870868 140273826584448 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1103 11:25:09.006528 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
            "I1103 11:25:09.006682 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num filters: 384\n",
            "I1103 11:25:09.006754 140273826584448 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num iterations: 8\n",
            "I1103 11:25:09.011896 140273826584448 efficientnet_model.py:148] round_filter input=32 output=64\n",
            "I1103 11:25:09.033799 140273826584448 efficientnet_model.py:148] round_filter input=32 output=64\n",
            "I1103 11:25:09.033942 140273826584448 efficientnet_model.py:148] round_filter input=16 output=32\n",
            "I1103 11:25:09.338113 140273826584448 efficientnet_model.py:148] round_filter input=16 output=32\n",
            "I1103 11:25:09.338264 140273826584448 efficientnet_model.py:148] round_filter input=24 output=48\n",
            "I1103 11:25:10.020237 140273826584448 efficientnet_model.py:148] round_filter input=24 output=48\n",
            "I1103 11:25:10.020410 140273826584448 efficientnet_model.py:148] round_filter input=40 output=80\n",
            "I1103 11:25:10.684973 140273826584448 efficientnet_model.py:148] round_filter input=40 output=80\n",
            "I1103 11:25:10.685141 140273826584448 efficientnet_model.py:148] round_filter input=80 output=160\n",
            "I1103 11:25:11.732532 140273826584448 efficientnet_model.py:148] round_filter input=80 output=160\n",
            "I1103 11:25:11.732707 140273826584448 efficientnet_model.py:148] round_filter input=112 output=224\n",
            "I1103 11:25:12.714320 140273826584448 efficientnet_model.py:148] round_filter input=112 output=224\n",
            "I1103 11:25:12.714481 140273826584448 efficientnet_model.py:148] round_filter input=192 output=384\n",
            "I1103 11:25:14.357067 140273826584448 efficientnet_model.py:148] round_filter input=192 output=384\n",
            "I1103 11:25:14.357228 140273826584448 efficientnet_model.py:148] round_filter input=320 output=640\n",
            "I1103 11:25:14.739020 140273826584448 efficientnet_model.py:148] round_filter input=1280 output=2560\n",
            "I1103 11:25:14.774692 140273826584448 efficientnet_model.py:462] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 28.08s\n",
            "I1103 11:25:14.935799 140273826584448 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 28.08s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "I1103 11:25:14.943616 140273826584448 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "I1103 11:25:14.945646 140273826584448 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "I1103 11:25:14.946308 140273826584448 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "I1103 11:25:14.947867 140273826584448 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTF2Test.test_session\n",
            "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "I1103 11:25:14.949512 140273826584448 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "I1103 11:25:14.950167 140273826584448 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "I1103 11:25:14.951327 140273826584448 test_util.py:1973] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 20 tests in 38.533s\n",
            "\n",
            "OK (skipped=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-EWBu4eO94b"
      },
      "source": [
        "To install on a custom machine check : [Installation](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_xmd22ToUsM"
      },
      "source": [
        "# Download the data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEiOFcHwoc-q"
      },
      "source": [
        "For this task we are going to be using the Oxford Pets dataset. This dataset contains 37 category pet dataset with roughly 200 images for each class. The annotations contain tight bounding box (ROI) around the head of the animal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd1OXlyjXIPl",
        "outputId": "b743f949-077f-4a3f-ee3e-80e23930d18d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#hide_output\n",
        "\n",
        "#Download the Oxford-IIIT Pet \n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "!tar -xf annotations.tar.gz\n",
        "!tar -xf images.tar.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-03 11:25:21--  http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 791918971 (755M) [application/x-gzip]\n",
            "Saving to: images.tar.gz\n",
            "\n",
            "images.tar.gz       100%[===================>] 755.23M  22.6MB/s    in 34s     \n",
            "\n",
            "2020-11-03 11:25:55 (22.1 MB/s) - images.tar.gz saved [791918971/791918971]\n",
            "\n",
            "--2020-11-03 11:25:55--  http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19173078 (18M) [application/x-gzip]\n",
            "Saving to: annotations.tar.gz\n",
            "\n",
            "annotations.tar.gz  100%[===================>]  18.28M  10.8MB/s    in 1.7s    \n",
            "\n",
            "2020-11-03 11:25:57 (10.8 MB/s) - annotations.tar.gz saved [19173078/19173078]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi5O2wG-7vUx",
        "outputId": "0fc84413-76b2-4bd5-b4cf-965f84e70dc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#hide\n",
        "!apt-get install tree"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 11 not upgraded.\n",
            "Need to get 40.7 kB of archives.\n",
            "After this operation, 105 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 40.7 kB in 0s (97.1 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 144628 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Msx5JA7Ahz5O"
      },
      "source": [
        "Before training let us create a folder **/content/workspace/**. \n",
        "\n",
        "It is within the **workspace** that we will store all our training set-ups. This will contain all files related to our model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwjV7RPIji4L"
      },
      "source": [
        "#Create the directory structure\n",
        "#We will store all the required files in the workspace folder\n",
        "!mkdir /content/workspace/\n",
        "!mkdir /content/workspace/images/ # store images\n",
        "!mkdir /content/workspace/annotations/ # store xml annotation files\n",
        "!mkdir /content/workspace/images/train # train images\n",
        "!mkdir /content/workspace/images/test # test images\n",
        "!mkdir /content/workspace/annotations/train # train annotations\n",
        "!mkdir /content/workspace/annotations/test # test annotations\n",
        "!mkdir /content/workspace/data/ # directory to store the tf_records & the label_map"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjwM6522iCxI",
        "outputId": "5e88907d-9fd8-4748-a7c8-0b35d69c0d52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#hide\n",
        "!tree -C \"/content/workspace\" --filelimit=100"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[01;34m/content/workspace\u001b[00m\n",
            " \u001b[01;34mannotations\u001b[00m\n",
            "  \u001b[01;34mtest\u001b[00m\n",
            "  \u001b[01;34mtrain\u001b[00m\n",
            " \u001b[01;34mdata\u001b[00m\n",
            " \u001b[01;34mimages\u001b[00m\n",
            "     \u001b[01;34mtest\u001b[00m\n",
            "     \u001b[01;34mtrain\u001b[00m\n",
            "\n",
            "7 directories, 0 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sil1Sw2xj-Lr"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "import logging\n",
        "import re\n",
        "import shutil\n",
        "import glob\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tarfile\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "IMAGE_DIR = \"/content/images\"\n",
        "ANNOT_DIR = \"/content/annotations/xmls\"\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "os.chdir(\"/content/\")\n",
        "\n",
        "%load_ext tensorboard\n",
        "%load_ext autoreload\n",
        "%matplotlib inline\n",
        "%autoreload 2"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kQUk7wSvwUI"
      },
      "source": [
        "import tensorflow.compat.v1 as tf1\n",
        "import contextlib2\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.utils import dataset_util\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import colab_utils\n",
        "from object_detection.builders import model_builder\n",
        "from object_detection.dataset_tools import tf_record_creation_util\n",
        "\n",
        "\n",
        "# Enable GPU dynamic memory allocation\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "#Suppress TensorFlow logging\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdQxI-c3pqNX"
      },
      "source": [
        "#Prepare Tensorflow 2 Object Detection Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH_CzhmQprdu"
      },
      "source": [
        "Tensorflow object detection API expects the data to be in the form of **TFRecords** . In this part we are going to convert our data present in **Pascal-VOC** format into **TFRecords**.  \n",
        "\n",
        "To do this we will implement the following the steps:\n",
        "\n",
        "* Iterate over all the annotations and partition the annotations into train and test datasets. The train annotatins and images will be saved to **/content/workspace/annotations/train** & **/content/workspace/images/train** respectively. Similarly the test data will be saved to **/content/workspace/annotations/test** & **/content/workspace/images/test** .\n",
        "\n",
        "* Convert all the `*.xml` annotation files into a single Pandas DataFrame object.\n",
        "\n",
        "* We will create a tensoflow 2 object detection format label-map which will be used in training/evaluation the model .\n",
        "\n",
        "* Use this Pandas DataFrame to create **TFRecords** for the train and test datasets. The **TFRecords** will be saved to **/content/workspace/data/**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WmTbGMPsH8J"
      },
      "source": [
        "## 1. **Partition the Dataset** :\n",
        "\n",
        "If we look at the data that is saved in **/content/images/** & **/content/annotations/** we will see that not all the images have the corresponding annotations and the images and annotations are saved as **{filename}.jpeg** & **{filename}.xml** respectively.\n",
        "\n",
        "What we will do is we will first split the images using `sklearn.train_test_split` into a train and test dataset. Then we will check for the corresponding annotation for the image . If the annotation file exists we will copy the image and annotation into their repectives directories under **/content/workspace** ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da9sXK5RkF7F"
      },
      "source": [
        "#Grab the list of all images\n",
        "all_images    = os.listdir(IMAGE_DIR)\n",
        "\n",
        "#Split the images into train and test datasets\n",
        "train_images, test_images = train_test_split(all_images, test_size=0.2, random_state = 123) \n",
        "\n",
        "#Grab the list of all the annotations for the train and test images\n",
        "#Some annotations may not exist we will filter these in the next cell\n",
        "train_xmls = [f.split(\".\")[0] + \".xml\" for f in train_images]\n",
        "test_xmls  = [f.split(\".\")[0] + \".xml\" for f in test_images ]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrgMBm63nK07",
        "outputId": "cc5e6337-def5-4dfe-b219-21e1bd27fa7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def move_file(fileList : list, src: str, dest: str):\n",
        "    \"\"\"\n",
        "    This Fn copy's files from a given fileList from src to dest\n",
        "    if the file exits.\n",
        "\n",
        "    Args:\n",
        "        fileList: List containing all the files present in the src directory.\n",
        "        src     : source directory for the files.\n",
        "        dest    : destination where to copy the files present in fileList.\n",
        "    \"\"\"\n",
        "    for f in tqdm(fileList):\n",
        "        fileName = os.path.join(src, f)\n",
        "        #Check if the file exits, if the file exits copy contents from src to dest\n",
        "        if os.path.exists(fileName):\n",
        "            shutil.copy2(src=fileName, dst=os.path.join(dest, f))\n",
        "\n",
        "\n",
        "\n",
        "#Move images and annotations to workspace directory\n",
        "move_file(train_images, src=IMAGE_DIR, dest=\"/content/workspace/images/train/\")\n",
        "move_file(test_images,  src=IMAGE_DIR, dest=\"/content/workspace/images/test/\")\n",
        "\n",
        "move_file(train_xmls, src=ANNOT_DIR, dest=\"/content/workspace/annotations/train/\")\n",
        "move_file(test_xmls,  src=ANNOT_DIR, dest=\"/content/workspace/annotations/test/\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 5914/5914 [00:01<00:00, 3151.13it/s]\n",
            "100%|| 1479/1479 [00:00<00:00, 2202.52it/s]\n",
            "100%|| 5914/5914 [00:00<00:00, 17253.55it/s]\n",
            "100%|| 1479/1479 [00:00<00:00, 20037.39it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c4skLSXuuBf"
      },
      "source": [
        "## 2. **Create Pandas DataFrame Object** :\n",
        "\n",
        "Now, that we have partitioned our dataset and the images/annotations are present in the repective directories we will now create a pandas dataframe from the `*.xml` files . The DataFrame will contrain the fillowing information:\n",
        "* **`filename`** `(str)`: Path to the image file.\n",
        "* **`width`**\t `(float/int)`: Absolute width of the image.\n",
        "* **`height`** `(float/int)`: Absolute height of the image.\n",
        "* **`labels`** `(str)`: The class of the object present in the bounding box.\t\n",
        "* **`xmin`** `(float/int)`: Absolute `xmin` co-ordinate for the bounding box.\n",
        "* **`ymin`** `(float/int)`: Absolute `ymin` co-ordinate for the bounding box.\n",
        "* **`xmax`** `(float/int)`: Absolute `xmax` co-ordinate for the bounding box.\t\n",
        "* **`ymax`** `(float/int)`: Absolute `ymax` co-ordinate for the bounding box.\n",
        "* **`encoded_label`** `(int)`: The label for the object in the bounding box. 0 represents always the background class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ujBfJs9YOy0"
      },
      "source": [
        "#Regular expression which will help us to extract the class of the object in the image\n",
        "#from the filename.\n",
        "exp = r\"/([^/]+)_\\d+.jpg$\"\n",
        "exp = re.compile(exp)\n",
        "\n",
        "#sklearn.LabelEncoder will be used to convert the class of the object into integer format.\n",
        "le  = LabelEncoder()\n",
        "\n",
        "def xml2pandas(annot_dir):\n",
        "    \"\"\"\n",
        "    Fn converts the xml files into a pandas dataframe.\n",
        "\n",
        "    Args:\n",
        "        annot_dir: Directory where all the *.xml annotation files are stored\n",
        "    \"\"\"\n",
        "    xml_list = []\n",
        "    for xml_file in tqdm(glob.glob(annot_dir + '/*.xml')):\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        for member in root.findall('object'):\n",
        "            value = (\n",
        "                root.find('filename').text,\n",
        "                int(root.find('size')[0].text),\n",
        "                int(root.find('size')[1].text),\n",
        "                member[0].text,\n",
        "                int(member[4][0].text),\n",
        "                int(member[4][1].text),\n",
        "                int(member[4][2].text),\n",
        "                int(member[4][3].text)\n",
        "                )\n",
        "            xml_list.append(value)\n",
        "        column_name = ['filename', 'width', 'height','labels', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "        xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
        "    logging.info(\"DataFrame Generated ! \")\n",
        "    return xml_df\n",
        "\n",
        "\n",
        "def process_data(annotDir, imageDir, image_set=\"train\"):\n",
        "    \"\"\"\n",
        "    Fn creates a pandas DataFrame object from the annotation in annotDir\n",
        "    and images in imageDir. This Fn also extracts the name of the class from the\n",
        "    filename and converts it into integer labels starting from 1 as 0 is reserved\n",
        "    for the background class always.\n",
        "\n",
        "    Args:\n",
        "        annotDir  : directory where the *.xml annotation files are stored.\n",
        "        imageDir  : directory where all the images are stored.\n",
        "        image_set : one of either `train` or `test`, this use when converting \n",
        "                    the class objects into integer formats.\n",
        "    \"\"\"\n",
        "    data = xml2pandas(annotDir)\n",
        "    #modify the filename to point to the original filename\n",
        "    data.filename = [os.path.join(imageDir, fname) for fname in data.filename.values]\n",
        "    #extract the class labels from the filenames\n",
        "    data[\"labels\"] = [exp.search(data.filename[idx]).group(1).lower() for idx in range(len(data))]\n",
        "    #encoded the labels into integers starting from 1\n",
        "    if image_set == \"train\" :\n",
        "        data[\"encoded_label\"] = le.fit_transform(data.labels) + 1\n",
        "    elif image_set == \"test\" :\n",
        "        data[\"encoded_label\"] = le.transform(data.labels) + 1\n",
        "    \n",
        "    return data"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USZgAbpmZRsC",
        "outputId": "733a6824-d219-4dd1-e2f3-b748a2bc7edb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Set up paths the train/test image and annotaion directory\n",
        "TRAIN_IMAGE_DIR = \"/content/workspace/images/train/\"\n",
        "TEST_IMAGE_DIR  = \"/content/workspace/images/test/\"\n",
        "TRAIN_ANNOTATION_DIR = \"/content/workspace/annotations/train/\"\n",
        "TEST_ANNOTATION_DIR  = \"/content/workspace/annotations/test/\"\n",
        "\n",
        "#Create pandas datafame from the *.xml files\n",
        "train_data = process_data(TRAIN_ANNOTATION_DIR, TRAIN_IMAGE_DIR, \"train\")\n",
        "test_data  = process_data(TEST_ANNOTATION_DIR, TEST_IMAGE_DIR, \"test\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 2982/2982 [00:08<00:00, 361.10it/s]\n",
            "100%|| 706/706 [00:00<00:00, 709.84it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaJPqQk7I7P8",
        "outputId": "c6c96ec3-136f-4d3f-a0f8-28473c35d536",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Cross check for missing files\n",
        "for f in train_data.filename:\n",
        "    if not os.path.exists(f):\n",
        "        #remove the missing file\n",
        "        print(f\"{f} is missing in train_data\")\n",
        "        train_data = train_data[train_data.filename != f]\n",
        "        train_data.reset_index(inplace=True, drop=True)\n",
        "\n",
        "\n",
        "for f in test_data.filename:\n",
        "    if not os.path.exists(f):\n",
        "        #remove the missing file\n",
        "        print(f\"{f} missing in test_data\")\n",
        "        test_data = test_data[test_data.filename != f]\n",
        "        test_data.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/workspace/images/train/Abyssinian_102.jpg is missing in train_data\n",
            "/content/workspace/images/test/Abyssinian_100.jpg missing in test_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G45XOUpayHN"
      },
      "source": [
        "**Our datasets are going to look something like this :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMOICsnka2vg"
      },
      "source": [
        "**The train_data :** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anGyy0wtHNh2",
        "outputId": "ef77962f-88f8-42df-963e-a244976a8e8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#hide_input\n",
        "\n",
        "#view the train dataset\n",
        "train_data.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>labels</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "      <th>encoded_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/workspace/images/train/Persian_191.jpg</td>\n",
              "      <td>500</td>\n",
              "      <td>333</td>\n",
              "      <td>persian</td>\n",
              "      <td>229</td>\n",
              "      <td>36</td>\n",
              "      <td>315</td>\n",
              "      <td>132</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/workspace/images/train/beagle_18.jpg</td>\n",
              "      <td>336</td>\n",
              "      <td>500</td>\n",
              "      <td>beagle</td>\n",
              "      <td>43</td>\n",
              "      <td>31</td>\n",
              "      <td>291</td>\n",
              "      <td>204</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/workspace/images/train/Sphynx_192.jpg</td>\n",
              "      <td>500</td>\n",
              "      <td>333</td>\n",
              "      <td>sphynx</td>\n",
              "      <td>334</td>\n",
              "      <td>20</td>\n",
              "      <td>412</td>\n",
              "      <td>109</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/workspace/images/train/boxer_181.jpg</td>\n",
              "      <td>500</td>\n",
              "      <td>333</td>\n",
              "      <td>boxer</td>\n",
              "      <td>259</td>\n",
              "      <td>8</td>\n",
              "      <td>362</td>\n",
              "      <td>112</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/workspace/images/train/Birman_126.jpg</td>\n",
              "      <td>334</td>\n",
              "      <td>500</td>\n",
              "      <td>birman</td>\n",
              "      <td>78</td>\n",
              "      <td>135</td>\n",
              "      <td>180</td>\n",
              "      <td>239</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          filename  width  ...  ymax encoded_label\n",
              "0  /content/workspace/images/train/Persian_191.jpg    500  ...   132            24\n",
              "1    /content/workspace/images/train/beagle_18.jpg    336  ...   204             5\n",
              "2   /content/workspace/images/train/Sphynx_192.jpg    500  ...   109            34\n",
              "3    /content/workspace/images/train/boxer_181.jpg    500  ...   112             9\n",
              "4   /content/workspace/images/train/Birman_126.jpg    334  ...   239             7\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHnhaFQJa50t"
      },
      "source": [
        "**The test_data :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGme3RrgmaZj",
        "outputId": "f0b4294a-134e-4ee6-e3b9-5048ee523f18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#hide_input\n",
        "\n",
        "#view the test dataset\n",
        "test_data.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>labels</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "      <th>encoded_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/workspace/images/test/Siamese_131.jpg</td>\n",
              "      <td>500</td>\n",
              "      <td>423</td>\n",
              "      <td>siamese</td>\n",
              "      <td>14</td>\n",
              "      <td>16</td>\n",
              "      <td>385</td>\n",
              "      <td>348</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/workspace/images/test/Bombay_115.jpg</td>\n",
              "      <td>600</td>\n",
              "      <td>428</td>\n",
              "      <td>bombay</td>\n",
              "      <td>44</td>\n",
              "      <td>87</td>\n",
              "      <td>234</td>\n",
              "      <td>335</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/workspace/images/test/Abyssinian_140.jpg</td>\n",
              "      <td>500</td>\n",
              "      <td>333</td>\n",
              "      <td>abyssinian</td>\n",
              "      <td>231</td>\n",
              "      <td>87</td>\n",
              "      <td>323</td>\n",
              "      <td>154</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/workspace/images/test/Russian_Blue_124.jpg</td>\n",
              "      <td>500</td>\n",
              "      <td>375</td>\n",
              "      <td>russian_blue</td>\n",
              "      <td>29</td>\n",
              "      <td>25</td>\n",
              "      <td>164</td>\n",
              "      <td>159</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/workspace/images/test/basset_hound_179.jpg</td>\n",
              "      <td>500</td>\n",
              "      <td>375</td>\n",
              "      <td>basset_hound</td>\n",
              "      <td>152</td>\n",
              "      <td>162</td>\n",
              "      <td>340</td>\n",
              "      <td>317</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              filename  ...  encoded_label\n",
              "0       /content/workspace/images/test/Siamese_131.jpg  ...             33\n",
              "1        /content/workspace/images/test/Bombay_115.jpg  ...              8\n",
              "2    /content/workspace/images/test/Abyssinian_140.jpg  ...              1\n",
              "3  /content/workspace/images/test/Russian_Blue_124.jpg  ...             28\n",
              "4  /content/workspace/images/test/basset_hound_179.jpg  ...              4\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqGEDw_uy9SA"
      },
      "source": [
        "## 3. **Create Label Map** :\n",
        "\n",
        "TensorFlow requires dataset to have a label map associated with it. This label map defines a mapping from string class names to integer class Ids. The label map should be a StringIntLabelMap text protobuf. Label map files have the extention `.pbtxt` and  we will place it under **/content/workspace/data** along with the **TFRecod** files which we will create in the next step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYOn6MZ2gWTV"
      },
      "source": [
        "#Create a label_map.pbtxt file : This label map is used both by the training and detection processes.\n",
        "unique_labels  = list(train_data.labels.unique())\n",
        "integer_labels = le.transform(unique_labels) + 1\n",
        "\n",
        "label_dict = {unique_labels[i] : integer_labels[i] for i in range(len(unique_labels))}\n",
        "\n",
        "label_map = \"/content/workspace/data/label_map.pbtxt\"\n",
        "categories = train_data.labels.unique()\n",
        "categories.sort()\t\n",
        "\n",
        "end = '\\n'\n",
        "s = ' '\n",
        "\n",
        "for name in categories:\n",
        "    out = ''\n",
        "    out += 'item' + s + '{' + end\n",
        "    out += s*2 + 'id:' + ' ' + (str(label_dict[name])) + end\n",
        "    out += s*2 + 'name:' + ' ' + '\\'' + name + '\\'' + end\n",
        "    out += '}' + end*2\n",
        "    \n",
        "    with open(label_map, 'a') as f:\n",
        "        f.write(out)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-2WZiy_rUxh"
      },
      "source": [
        "Our **`label_map.pbtxt`** file will look like this :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vat0S2CNN9v4"
      },
      "source": [
        "```\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'abyssinian'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 2\n",
        "  name: 'american_bulldog'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 3\n",
        "  name: 'american_pit_bull_terrier'\n",
        "}\n",
        "\n",
        "item {\n",
        "  id: 4\n",
        "  name: 'basset_hound'\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U2E5iSCN-Cf"
      },
      "source": [
        "The **`label_map.pbtxt`** file has been placed under **/content/workspace/data/label_map.pbtxt**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPUXb9D00XtV"
      },
      "source": [
        "## 4. **Create TensorFlow Records** :\n",
        "\n",
        "In this step we will convert our annotatinos present in the pandas dataframe object into **TFRecord** format.\n",
        "\n",
        "For every example in our dataset, we should have the following information:\n",
        "- An RGB image for the dataset encoded as jpeg or png.\n",
        "- A bounding box coordinates for each image `(with origin in top left corner)` defined by 4 floating point numbers `[ymin, xmin, ymax, xmax]`.\n",
        "- The class of the object in the bounding box."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q21ITaCVkeqz"
      },
      "source": [
        "> Note: For the bounding-boxes, the normalized coordinates (x / width, y / height) are stored in the **TFRecord** dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX4lS24Dkb4w"
      },
      "source": [
        "Since our dataset has more than a fairly large number of annotations we will shard your dataset into multiple files.\n",
        "Instead of writing all tf.Example protos to a single file we will store the dataset into multiple files .\n",
        "\n",
        "Our dataset is going to look something like this:\n",
        "\n",
        "```bash\n",
        "/{directory_path}/dataset.record-00000-00010\n",
        "/{directory_path}/dataset.record-00001-00010\n",
        "...\n",
        "/{directory_path}/dataset.record-00009-00010\n",
        "```\n",
        "\n",
        "Our train dataset is going to be stored as :\n",
        "\n",
        "```bash\n",
        "/content/workspace/data/train.record-00000-of-00010\n",
        "/content/workspace/data/train.record-00001-of-00010\n",
        "...\n",
        "/content/workspace/data/train.record-00009-of-00010\n",
        "```\n",
        "\n",
        "Similary for the test dataset :\n",
        "\n",
        "```bash\n",
        "/content/workspace/data/test.record-00000-of-00010\n",
        "/content/workspace/data/test.record-00001-of-00010\n",
        "...\n",
        "/content/workspace/data/test.record-00009-of-00010\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRbq450WraVj"
      },
      "source": [
        "def create_tf_example(fname, data):\n",
        "    \"\"\"\n",
        "    Creates a tf.Example proto from a single image\n",
        "    from the given data\n",
        "\n",
        "    Args:\n",
        "        fname: filename of a single image from data.\n",
        "        data : a pandas dataframe object in the format \n",
        "               specified in step 2. \n",
        "\n",
        "    Returns:\n",
        "        example: The created tf.Example.\n",
        "    \"\"\"\n",
        "    curr_data = data.loc[data.filename == fname]\n",
        "    \n",
        "    filename = fname.encode('utf8') # Filename of the image\n",
        "    height = curr_data[\"height\"].values[0] # Image height\n",
        "    width = curr_data[\"width\"].values[0] # Image width\n",
        "    \n",
        "    image_format = b'jpeg' # b'jpeg' or b'png'\n",
        "    \n",
        "    # List of normalized left x coordinates in bounding box (1 per box).\n",
        "    xmins = list(curr_data[\"xmin\"].values / width) \n",
        "    # List of normalized right x coordinates in bounding box (1 per box).\n",
        "    xmaxs = list(curr_data[\"xmax\"].values / width) \n",
        "    # List of normalized top y coordinates in bounding box (1 per box).\n",
        "    ymins = list(curr_data[\"ymin\"].values / height)\n",
        "    # List of normalized bottom y coordinates in bounding box (1 per box).\n",
        "    ymaxs = list(curr_data[\"ymax\"].values / height)\n",
        "    \n",
        "    # List of string class name of bounding box (1 per box)\n",
        "    classes_text = list(curr_data[\"labels\"].values)\n",
        "    classes_text = [text.encode('utf8') for text in classes_text]\n",
        "    \n",
        "    # List of integer class id of bounding box (1 per box)\n",
        "    classes = list(curr_data[\"encoded_label\"].values) \n",
        "\n",
        "    with tf1.gfile.GFile(filename, 'rb') as fid:\n",
        "        encoded_image_data = fid.read() # Encoded image bytes\n",
        "\n",
        "    features = tf1.train.Example(features=tf1.train.Features(feature={\n",
        "      'image/height': dataset_util.int64_feature(height),\n",
        "      'image/width': dataset_util.int64_feature(width),\n",
        "      'image/filename': dataset_util.bytes_feature(filename),\n",
        "      'image/source_id': dataset_util.bytes_feature(filename),\n",
        "      'image/encoded': dataset_util.bytes_feature(encoded_image_data),\n",
        "      'image/format': dataset_util.bytes_feature(image_format),\n",
        "      'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "      'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "      'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "      'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "      'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "      }))\n",
        "    \n",
        "    return features\n",
        "\n",
        "\n",
        "def create_records(output_path, data, shards=10):\n",
        "    \"\"\"\n",
        "    Fn iterates over all the annotations in dataset and creates a \n",
        "    sharded TFRecord dataset and additionally saves the sharded TFRecord dataset\n",
        "    to output path.\n",
        "\n",
        "    Args:\n",
        "        output_path: Path where to save the dataset\n",
        "        data       : A pandas Dataframe object as specified in step-2.\n",
        "        shards     : Number of the shards over which to save the dataset.\n",
        "                     The dataset is going to saved inside `shards` no. of files.\n",
        "    \"\"\"\n",
        "    writer = tf1.python_io.TFRecordWriter(output_path)\n",
        "    fnames = list(data.filename.unique())\n",
        "\n",
        "    with contextlib2.ExitStack() as tf_record_close_stack:\n",
        "        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(tf_record_close_stack,output_path,shards)\n",
        "        #enumerate over all the unique images present in the dataset\n",
        "        #and create a tf.Example proto for the particular annotations.\n",
        "        for index, fname in enumerate(fnames):\n",
        "            tf_example = create_tf_example(fname, data)\n",
        "            output_shard_index = index % shards\n",
        "            output_tfrecords[output_shard_index].write(tf_example.SerializeToString())"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6jQCjuZyv1R",
        "outputId": "c7b490a8-790e-4833-fb55-68dcd16c72c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Generate the TF Records for the train and test dataset\n",
        "print(\"Creating TFRecords ..... \", end='')\n",
        "start_time = time.time()\n",
        "\n",
        "create_records(\"/content/workspace/data/train.record\", data=train_data)\n",
        "create_records(\"/content/workspace/data/test.record\",  data=test_data )\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print('Done! Took {} seconds'.format(elapsed_time))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating TFRecords ..... Done! Took 4.774616956710815 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS-y0jUN49o1"
      },
      "source": [
        "Our dataset is now prepared for training using a model from **[TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)** . \n",
        "\n",
        "The directory structure for the workspace should look something like this at this stage:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qLxMnF07U8_",
        "outputId": "e28a3421-3836-46f9-8a4d-858300bd6551",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#hide_input\n",
        "!tree -C \"/content/workspace\" --filelimit=100"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[01;34m/content/workspace\u001b[00m\n",
            " \u001b[01;34mannotations\u001b[00m\n",
            "  \u001b[01;34mtest\u001b[00m [706 entries exceeds filelimit, not opening dir]\n",
            "  \u001b[01;34mtrain\u001b[00m [2982 entries exceeds filelimit, not opening dir]\n",
            " \u001b[01;34mdata\u001b[00m\n",
            "  label_map.pbtxt\n",
            "  test.record\n",
            "  test.record-00000-of-00010\n",
            "  test.record-00001-of-00010\n",
            "  test.record-00002-of-00010\n",
            "  test.record-00003-of-00010\n",
            "  test.record-00004-of-00010\n",
            "  test.record-00005-of-00010\n",
            "  test.record-00006-of-00010\n",
            "  test.record-00007-of-00010\n",
            "  test.record-00008-of-00010\n",
            "  test.record-00009-of-00010\n",
            "  train.record\n",
            "  train.record-00000-of-00010\n",
            "  train.record-00001-of-00010\n",
            "  train.record-00002-of-00010\n",
            "  train.record-00003-of-00010\n",
            "  train.record-00004-of-00010\n",
            "  train.record-00005-of-00010\n",
            "  train.record-00006-of-00010\n",
            "  train.record-00007-of-00010\n",
            "  train.record-00008-of-00010\n",
            "  train.record-00009-of-00010\n",
            " \u001b[01;34mimages\u001b[00m\n",
            "     \u001b[01;34mtest\u001b[00m [1479 entries exceeds filelimit, not opening dir]\n",
            "     \u001b[01;34mtrain\u001b[00m [5914 entries exceeds filelimit, not opening dir]\n",
            "\n",
            "7 directories, 23 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJMZL6EL9pgm"
      },
      "source": [
        "# Configure Custom TensorFlow2 Object Detection Training Configuration\n",
        "\n",
        "> In this section we will download a pretrained-model from the [TF2 OD model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) and set up out training configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRCP0Jld97t1"
      },
      "source": [
        "In this tutorial we are going to implement the lightweight, smallest state of the art **efficientdet model**. \n",
        "\n",
        "\n",
        "We will create a directory call pretrained-models in our wokspace folder.\n",
        "\n",
        "We will download the latest pre-trained network for the model we wish to use. This can be in [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md).\n",
        "\n",
        "Once the ***.tar.gz** file has been downloaded, we will extract the file contents into. **/content/workspace/pre-trained-models** ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxZVSbqtzv_5",
        "outputId": "4f591ab3-6294-4720-db2f-7cfea3520875",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#hide_output\n",
        "\n",
        "# Download the latest-pretrained weights for the efficientdet_d0 model and the config file\n",
        "\n",
        "#LINK : http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz\n",
        "model_name = \"efficientdet_d0_coco17_tpu-32\"\n",
        "model = \"efficientdet_d0_coco17_tpu-32.tar.gz\"\n",
        "\n",
        "os.makedirs(\"/content/workspace/pre_trained_models/\", exist_ok=True)\n",
        "download_tar = f\"http://download.tensorflow.org/models/object_detection/tf2/20200711/{model}\"\n",
        "!wget {download_tar} -P \"/content/workspace/pre_trained_models/\"\n",
        "\n",
        "tar = tarfile.open(f\"/content/workspace/pre_trained_models/{model}\")\n",
        "tar.extractall(path=\"/content/workspace/pre_trained_models/\")\n",
        "tar.close()\n",
        "\n",
        "os.unlink(f\"/content/workspace/pre_trained_models/{model}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-03 11:33:06--  http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 74.125.195.128, 2607:f8b0:400e:c08::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|74.125.195.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30736482 (29M) [application/x-tar]\n",
            "Saving to: /content/workspace/pre_trained_models/efficientdet_d0_coco17_tpu-32.tar.gz\n",
            "\n",
            "efficientdet_d0_coc 100%[===================>]  29.31M   133MB/s    in 0.2s    \n",
            "\n",
            "2020-11-03 11:33:06 (133 MB/s) - /content/workspace/pre_trained_models/efficientdet_d0_coco17_tpu-32.tar.gz saved [30736482/30736482]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y8eJm0L9nQB"
      },
      "source": [
        "The directory structure for the workspace should look something like this at this stage:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXXh6qwOKpVK",
        "outputId": "140ab4a9-9cde-4ed0-cd5c-adc2ef90cbe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#hide_input\n",
        "!tree -C \"/content/workspace\" --filelimit=100"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[01;34m/content/workspace\u001b[00m\n",
            " \u001b[01;34mannotations\u001b[00m\n",
            "  \u001b[01;34mtest\u001b[00m [706 entries exceeds filelimit, not opening dir]\n",
            "  \u001b[01;34mtrain\u001b[00m [2982 entries exceeds filelimit, not opening dir]\n",
            " \u001b[01;34mdata\u001b[00m\n",
            "  label_map.pbtxt\n",
            "  test.record\n",
            "  test.record-00000-of-00010\n",
            "  test.record-00001-of-00010\n",
            "  test.record-00002-of-00010\n",
            "  test.record-00003-of-00010\n",
            "  test.record-00004-of-00010\n",
            "  test.record-00005-of-00010\n",
            "  test.record-00006-of-00010\n",
            "  test.record-00007-of-00010\n",
            "  test.record-00008-of-00010\n",
            "  test.record-00009-of-00010\n",
            "  train.record\n",
            "  train.record-00000-of-00010\n",
            "  train.record-00001-of-00010\n",
            "  train.record-00002-of-00010\n",
            "  train.record-00003-of-00010\n",
            "  train.record-00004-of-00010\n",
            "  train.record-00005-of-00010\n",
            "  train.record-00006-of-00010\n",
            "  train.record-00007-of-00010\n",
            "  train.record-00008-of-00010\n",
            "  train.record-00009-of-00010\n",
            " \u001b[01;34mimages\u001b[00m\n",
            "  \u001b[01;34mtest\u001b[00m [1479 entries exceeds filelimit, not opening dir]\n",
            "  \u001b[01;34mtrain\u001b[00m [5914 entries exceeds filelimit, not opening dir]\n",
            " \u001b[01;34mpre_trained_models\u001b[00m\n",
            "     \u001b[01;34mefficientdet_d0_coco17_tpu-32\u001b[00m\n",
            "         \u001b[01;34mcheckpoint\u001b[00m\n",
            "          checkpoint\n",
            "          ckpt-0.data-00000-of-00001\n",
            "          ckpt-0.index\n",
            "         pipeline.config\n",
            "         \u001b[01;34msaved_model\u001b[00m\n",
            "             \u001b[01;34massets\u001b[00m\n",
            "             saved_model.pb\n",
            "             \u001b[01;34mvariables\u001b[00m\n",
            "                 variables.data-00000-of-00001\n",
            "                 variables.index\n",
            "\n",
            "13 directories, 30 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycUDtDdn_KNj"
      },
      "source": [
        "Now that we have downloaded and extracted our pre-trained model, lets create a directory for our training job. Under the **/content/workspace/** create a new directory named **models** this will be the folder where  we will store all the configurations, model_checkpoints, logs for our custom trained model.\n",
        "\n",
        "Under the **/content/workspace/models/** dir create a dir named as **efficientdet_d0_coco17_tpu-32** and copy the **/content/workspace/pre-trained-models/efficientdet_d0_coco17_tpu-32/pipeline.config** file inside the newly created directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mx-x2HG7Fmh",
        "outputId": "b0e4e32c-8e19-49f6-a5c7-8f0210b8b45c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "os.makedirs(\"/content/workspace/models/\", exist_ok=True)\n",
        "os.makedirs(f\"/content/workspace/models/{model_name}\", exist_ok=True)\n",
        "\n",
        "config_path = f\"/content/workspace/pre_trained_models/{model_name}/pipeline.config\"\n",
        "shutil.copy2(config_path, f\"/content/workspace/models/{model_name}\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/workspace/models/efficientdet_d0_coco17_tpu-32/pipeline.config'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWqCYtziAh7F"
      },
      "source": [
        "Each model has a model_name, a pipeline.config file, a pretrained_checkpoint. \n",
        "\n",
        "The pipeline.config file is a shell of a training configuration specific to each model type, provided by the authors of the TF2 OD repository. \n",
        "\n",
        "The pretrained_checkpoint is the location of a pretrained weights file saved from when the object detection model was pretrained on the COCO dataset. \n",
        "\n",
        "We will start from these weights, and then fine tune into our particular custom dataset task. By using pretraining, our model does not need to start from square one in identifying what features might be useful for object detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgqsAAGpACTt"
      },
      "source": [
        "We will map our training data files to variables for use in our computer vision training pipeline configuration. \n",
        "\n",
        "We will now edit the **/content/workspace/models/pipeline.config** to point to our custom data, the pretrained_checkpoint, and we also specify some training parameters. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt07xU0F9Dfp",
        "outputId": "6112d691-a7e8-49b3-c4d6-c444b6ea1f75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Path to train and test TFRecords\n",
        "test_record_fname = \"/content/workspace/data/test.record-?????-of-00010\"\n",
        "train_record_fname = \"/content/workspace/data/train.record-?????-of-00010\"\n",
        "\n",
        "#Path to the TensorFlow Object Detection format label_map\n",
        "label_map_pbtxt_fname = \"/content/workspace/data/label_map.pbtxt\"\n",
        "\n",
        "#Path to the pipeline.config file\n",
        "config_path = f\"/content/workspace/models/{model_name}/pipeline.config\"\n",
        "\n",
        "#Path to the pretrained model checkpoints \n",
        "fine_tune = f\"/content/workspace/pre_trained_models/{model_name}/checkpoint/ckpt-0\"\n",
        "\n",
        "#if you can fit a large batch in memory, it may speed up your training\n",
        "batch_size = 16\n",
        "#The more steps, the longer the training\n",
        "num_steps =  50000\n",
        "\n",
        "model_dir = f\"/content/workspace/models/{model_name}\"\n",
        "\n",
        "\n",
        "def get_num_classes(pbtxt_fname):\n",
        "    \"\"\"Get total number of classes from label_map.pbtxt file\"\"\"\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())\n",
        "\n",
        "\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "\n",
        "print(\"CUSTOM CONFIGURATION PARAMETERS : \")\n",
        "print(\"Config Path: \", config_path)\n",
        "print(\"Checkpoint Path: \", fine_tune)\n",
        "print(\"Label Map: \", label_map_pbtxt_fname)\n",
        "print(\"Train TFRecords: \", train_record_fname)\n",
        "print(\"Test TFRecords: \", test_record_fname)\n",
        "print(\"Total Steps: \", num_steps)\n",
        "print(\"Num classes: \", num_classes)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUSTOM CONFIGURATION PARAMETERS : \n",
            "Config Path:  /content/workspace/models/efficientdet_d0_coco17_tpu-32/pipeline.config\n",
            "Checkpoint Path:  /content/workspace/pre_trained_models/efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0\n",
            "Label Map:  /content/workspace/data/label_map.pbtxt\n",
            "Train TFRecords:  /content/workspace/data/train.record-?????-of-00010\n",
            "Test TFRecords:  /content/workspace/data/test.record-?????-of-00010\n",
            "Total Steps:  50000\n",
            "Num classes:  37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VaPrHNz9eJ5"
      },
      "source": [
        "#write custom configuration file by slotting our dataset, model checkpoint, and training parameters into the base pipeline file\n",
        "\n",
        "with open(config_path) as f:\n",
        "    s = f.read()\n",
        "\n",
        "with open(config_path, 'w') as f:\n",
        "    # fine_tune_checkpoint\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"', 'fine_tune_checkpoint: \"{}\"'.format(fine_tune), s)\n",
        "    # tfrecord files train and test\n",
        "    s = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    s = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n",
        "    # label_map_path\n",
        "    s = re.sub('label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "    # Set training batch_size.\n",
        "    s = re.sub('batch_size: [0-9]+','batch_size: {}'.format(batch_size), s)\n",
        "    # Set training steps, num_steps\n",
        "    s = re.sub('num_steps: [0-9]+', 'num_steps: {}'.format(num_steps), s)\n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('num_classes: [0-9]+','num_classes: {}'.format(num_classes), s)\n",
        "    #fine-tune checkpoint type\n",
        "    s = re.sub('fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n",
        "        \n",
        "    f.write(s)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-a2hJfS_qEe",
        "outputId": "b75be7ec-e3a8-4048-92d0-1ab012f9bd28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#hide\n",
        "!cat {config_path}"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model {\n",
            "  ssd {\n",
            "    num_classes: 37\n",
            "    image_resizer {\n",
            "      keep_aspect_ratio_resizer {\n",
            "        min_dimension: 512\n",
            "        max_dimension: 512\n",
            "        pad_to_max_dimension: true\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: \"ssd_efficientnet-b0_bifpn_keras\"\n",
            "      conv_hyperparams {\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 3.9999998989515007e-05\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          truncated_normal_initializer {\n",
            "            mean: 0.0\n",
            "            stddev: 0.029999999329447746\n",
            "          }\n",
            "        }\n",
            "        activation: SWISH\n",
            "        batch_norm {\n",
            "          decay: 0.9900000095367432\n",
            "          scale: true\n",
            "          epsilon: 0.0010000000474974513\n",
            "        }\n",
            "        force_use_bias: true\n",
            "      }\n",
            "      bifpn {\n",
            "        min_level: 3\n",
            "        max_level: 7\n",
            "        num_iterations: 3\n",
            "        num_filters: 64\n",
            "      }\n",
            "    }\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 1.0\n",
            "        x_scale: 1.0\n",
            "        height_scale: 1.0\n",
            "        width_scale: 1.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "        use_matmul_gather: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      weight_shared_convolutional_box_predictor {\n",
            "        conv_hyperparams {\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 3.9999998989515007e-05\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            random_normal_initializer {\n",
            "              mean: 0.0\n",
            "              stddev: 0.009999999776482582\n",
            "            }\n",
            "          }\n",
            "          activation: SWISH\n",
            "          batch_norm {\n",
            "            decay: 0.9900000095367432\n",
            "            scale: true\n",
            "            epsilon: 0.0010000000474974513\n",
            "          }\n",
            "          force_use_bias: true\n",
            "        }\n",
            "        depth: 64\n",
            "        num_layers_before_predictor: 3\n",
            "        kernel_size: 3\n",
            "        class_prediction_bias_init: -4.599999904632568\n",
            "        use_depthwise: true\n",
            "      }\n",
            "    }\n",
            "    anchor_generator {\n",
            "      multiscale_anchor_generator {\n",
            "        min_level: 3\n",
            "        max_level: 7\n",
            "        anchor_scale: 4.0\n",
            "        aspect_ratios: 1.0\n",
            "        aspect_ratios: 2.0\n",
            "        aspect_ratios: 0.5\n",
            "        scales_per_octave: 3\n",
            "      }\n",
            "    }\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 9.99999993922529e-09\n",
            "        iou_threshold: 0.5\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    loss {\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      classification_loss {\n",
            "        weighted_sigmoid_focal {\n",
            "          gamma: 1.5\n",
            "          alpha: 0.25\n",
            "        }\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    encode_background_as_zeros: true\n",
            "    normalize_loc_loss_by_codesize: true\n",
            "    inplace_batchnorm_update: true\n",
            "    freeze_batchnorm: false\n",
            "    add_background_class: false\n",
            "  }\n",
            "}\n",
            "train_config {\n",
            "  batch_size: 16\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    random_scale_crop_and_pad_to_square {\n",
            "      output_size: 512\n",
            "      scale_min: 0.10000000149011612\n",
            "      scale_max: 2.0\n",
            "    }\n",
            "  }\n",
            "  sync_replicas: true\n",
            "  optimizer {\n",
            "    momentum_optimizer {\n",
            "      learning_rate {\n",
            "        cosine_decay_learning_rate {\n",
            "          learning_rate_base: 0.07999999821186066\n",
            "          total_steps: 300000\n",
            "          warmup_learning_rate: 0.0010000000474974513\n",
            "          warmup_steps: 2500\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.8999999761581421\n",
            "    }\n",
            "    use_moving_average: false\n",
            "  }\n",
            "  fine_tune_checkpoint: \"/content/workspace/pre_trained_models/efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0\"\n",
            "  num_steps: 50000\n",
            "  startup_delay_steps: 0.0\n",
            "  replicas_to_aggregate: 8\n",
            "  max_number_of_boxes: 100\n",
            "  unpad_groundtruth_tensors: false\n",
            "  fine_tune_checkpoint_type: \"detection\"\n",
            "  use_bfloat16: true\n",
            "  fine_tune_checkpoint_version: V2\n",
            "}\n",
            "train_input_reader: {\n",
            "  label_map_path: \"/content/workspace/data/label_map.pbtxt\"\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/workspace/data/train.record-?????-of-00010\"\n",
            "  }\n",
            "}\n",
            "\n",
            "eval_config: {\n",
            "  metrics_set: \"coco_detection_metrics\"\n",
            "  use_moving_averages: false\n",
            "  batch_size: 16;\n",
            "}\n",
            "\n",
            "eval_input_reader: {\n",
            "  label_map_path: \"/content/workspace/data/label_map.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_epochs: 1\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/workspace/data/test.record-?????-of-00010\"\n",
            "  }\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0oZJ90N5CYD"
      },
      "source": [
        "The modified config file will be saved as **/content/workspace/models/efficientdet_d0_coco17_tpu-32/pipeline.config**\n",
        "\n",
        "Let's check the directory structure :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09SeMUnY5HA4",
        "outputId": "41b84e34-2047-4679-8cc7-a9253331ab46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#hide_input\n",
        "!tree -C \"/content/workspace\" --filelimit=100"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[01;34m/content/workspace\u001b[00m\n",
            " \u001b[01;34mannotations\u001b[00m\n",
            "  \u001b[01;34mtest\u001b[00m [706 entries exceeds filelimit, not opening dir]\n",
            "  \u001b[01;34mtrain\u001b[00m [2982 entries exceeds filelimit, not opening dir]\n",
            " \u001b[01;34mdata\u001b[00m\n",
            "  label_map.pbtxt\n",
            "  test.record\n",
            "  test.record-00000-of-00010\n",
            "  test.record-00001-of-00010\n",
            "  test.record-00002-of-00010\n",
            "  test.record-00003-of-00010\n",
            "  test.record-00004-of-00010\n",
            "  test.record-00005-of-00010\n",
            "  test.record-00006-of-00010\n",
            "  test.record-00007-of-00010\n",
            "  test.record-00008-of-00010\n",
            "  test.record-00009-of-00010\n",
            "  train.record\n",
            "  train.record-00000-of-00010\n",
            "  train.record-00001-of-00010\n",
            "  train.record-00002-of-00010\n",
            "  train.record-00003-of-00010\n",
            "  train.record-00004-of-00010\n",
            "  train.record-00005-of-00010\n",
            "  train.record-00006-of-00010\n",
            "  train.record-00007-of-00010\n",
            "  train.record-00008-of-00010\n",
            "  train.record-00009-of-00010\n",
            " \u001b[01;34mimages\u001b[00m\n",
            "  \u001b[01;34mtest\u001b[00m [1479 entries exceeds filelimit, not opening dir]\n",
            "  \u001b[01;34mtrain\u001b[00m [5914 entries exceeds filelimit, not opening dir]\n",
            " \u001b[01;34mmodels\u001b[00m\n",
            "  \u001b[01;34mefficientdet_d0_coco17_tpu-32\u001b[00m\n",
            "      pipeline.config\n",
            " \u001b[01;34mpre_trained_models\u001b[00m\n",
            "     \u001b[01;34mefficientdet_d0_coco17_tpu-32\u001b[00m\n",
            "         \u001b[01;34mcheckpoint\u001b[00m\n",
            "          checkpoint\n",
            "          ckpt-0.data-00000-of-00001\n",
            "          ckpt-0.index\n",
            "         pipeline.config\n",
            "         \u001b[01;34msaved_model\u001b[00m\n",
            "             \u001b[01;34massets\u001b[00m\n",
            "             saved_model.pb\n",
            "             \u001b[01;34mvariables\u001b[00m\n",
            "                 variables.data-00000-of-00001\n",
            "                 variables.index\n",
            "\n",
            "15 directories, 31 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWA3pzVPErKV"
      },
      "source": [
        "# Train Custom TF2 Object Detector\n",
        "\n",
        "\n",
        "To initiate a new training job, we need to run the script **/content/models/research/object_detection/model_main_tf2.py** \n",
        "\n",
        "\n",
        "* config_path: path to the configuration file defined above in writing custom training configuration.\n",
        "\n",
        "* model_dir: the location tensorboard logs and saved model checkpoints will save to"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVPCaSGpBz_R"
      },
      "source": [
        "#hide_output\n",
        "\n",
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={config_path} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGGLw7DDFGnH"
      },
      "source": [
        "To evaluate our model on COCO-Evaluation metrics we need to run the script **/content/models/research/object_detection/model_main_tf2.py** .\n",
        "\n",
        "\n",
        "> Note: This process automatically evaluates the model on the latest checkpoints that the training job generates. So we can also run this script in the backgound as our model keeps training and as checkpoints are generated the script will automatically evaluate the model on the COCO-metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9R2gYK2CW6Y"
      },
      "source": [
        "#hide_output\n",
        "\n",
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={config_path} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --checkpoint_dir={model_dir} \\\n",
        "    --alsologtostderr\n",
        "    --eval_timeout=10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHgQon_QF1C6"
      },
      "source": [
        "**Monitor Training Job Progress using TensorBoard:**\n",
        "\n",
        "We can either use one of the 2 commands:\n",
        "\n",
        "\n",
        "To open in a terminal :\n",
        "```bash\n",
        "tensorboard --logdir \"/content/workspace/models/efficientdet_d0_coco17_tpu-32/\n",
        "```\n",
        "\n",
        "For a Jupyter-Environment:\n",
        "```python\n",
        "%tensorboard --logdir \"/content/workspace/models/efficientdet_d0_coco17_tpu-32/\"\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q91mdKBUHCNj"
      },
      "source": [
        "We will have logs that are going to look similar to this :\n",
        "\n",
        "![logs](https://www.dropbox.com/s/b3pvc7po59xcvmc/demo_logs.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy2dU3wacdox"
      },
      "source": [
        "#hide\n",
        "%tensorboard --logdir \"/content/workspace/models/efficientdet_d0_coco17_tpu-32/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywjzMFyJJlLB"
      },
      "source": [
        "#Exporting a Trained Inference Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DitMUiA9JuEk"
      },
      "source": [
        "Once your training job is complete, you need to extract the newly trained inference graph, which will be later used to perform the object detection. This can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HI084X_KWBi"
      },
      "source": [
        "#see where our model saved weights\n",
        "%ls \"/content/workspace/models/efficientdet_d0_coco17_tpu-32/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcMaaY3nJ7Jr"
      },
      "source": [
        "# make a directory to save the exported graph\n",
        "os.makedirs(\"/content/workspace/exported_models/efficientdet_d0\", exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpmRU3r7cjCB"
      },
      "source": [
        "#Once your training job is complete, you need to extract the newly trained inference graph, \n",
        "#which will be later used to perform the object detection\n",
        "\n",
        "#path to save the exporter inference graph\n",
        "output_directory = \"/content/workspace/exported_models/efficientdet_d0\"\n",
        "\n",
        "#path to trained model checkpoints\n",
        "checkpoint_dir = \"/content/workspace/models/efficientdet_d0_coco17_tpu-32/\"\n",
        "\n",
        "#run script to export model weights\n",
        "!python /content/models/research/object_detection/exporter_main_v2.py \\\n",
        "    --trained_checkpoint_dir {checkpoint_dir} \\\n",
        "    --output_directory {output_directory} \\\n",
        "    --pipeline_config_path {config_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KUf6m8WJrYL"
      },
      "source": [
        "Let's checkout our directory structure :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNamBtC9c4o5"
      },
      "source": [
        "#hide_input\n",
        "!tree -C \"/content/workspace\" --filelimit=100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suER8t1qKfMZ"
      },
      "source": [
        "In the next part we are going to use this exported graph to do inference on custom images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06eYsRhyKeHB"
      },
      "source": [
        "# Run Inference on Test Images with Custom TensorFlow2 Object Detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CnKdvvgMFSB"
      },
      "source": [
        "In this section we will run inference on images using our Custom TensorFlow2 Object Detector exported graph\n",
        "\n",
        "\n",
        "To run inference we will create a few helper functions first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIHwthwxdzHX"
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: the file path to the image\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "  image = Image.open(BytesIO(img_data))\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "def load_model(model_dir):\n",
        "    model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
        "    model = tf.saved_model.load(str(model_dir))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEJm7IBmMWQ2"
      },
      "source": [
        "Load in the **category_index**, which is a dictionary mapping of the classes and the index labels & the **Custom TensorFlow2 Object Detector** from the exported graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQlDZDbqea3e"
      },
      "source": [
        "#path to the label map\n",
        "PATH_TO_LABELS = \"/content/workspace/data/label_map.pbtxt\"\n",
        "#generate a category index dictionary from the label map\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
        "\n",
        "#path to the images from the test directory we will reun inference on these images\n",
        "PATH_TO_TEST_IMAGES_DIR = pathlib.Path(\"/content/workspace/images/test/\")\n",
        "TEST_IMAGE_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob(\"*.jpg\")))\n",
        "\n",
        "\n",
        "#Load model from the exported model graph\n",
        "print('Loading model...', end='')\n",
        "start_time = time.time()\n",
        "detection_model = load_model(\"/content/workspace/exported_models/efficientdet_d0\")\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print('Done! Took {} seconds'.format(elapsed_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8Qh4npjfJiV"
      },
      "source": [
        "def run_inference_for_single_image(model, image):\n",
        "    \"\"\"\n",
        "    Fn to run inference on a single image\n",
        "    \"\"\"\n",
        "    image = np.asarray(image)\n",
        "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "    input_tensor = tf.convert_to_tensor(image)\n",
        "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "    input_tensor = input_tensor[tf.newaxis,...]\n",
        "    # Run inference\n",
        "    model_fn = model.signatures['serving_default']\n",
        "    output_dict = model_fn(input_tensor)\n",
        "    \n",
        "    # All outputs are batches tensors.\n",
        "    # Convert to numpy arrays, and take index [0] to remove the batch dimension\n",
        "    # We're only interested in the first num_detections.\n",
        "    num_detections = int(output_dict.pop('num_detections'))\n",
        "    output_dict = {key:value[0, :num_detections].numpy() for key,value in output_dict.items()}\n",
        "    output_dict['num_detections'] = num_detections\n",
        "    # detection_classes should be ints.\n",
        "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
        "    \n",
        "    # Handle models with masks\n",
        "    if 'detection_masks' in output_dict:\n",
        "        # Reframe the the bbox mask to the image size.\n",
        "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "            output_dict['detection_masks'], output_dict['detection_boxes'],\n",
        "            image.shape[0], \n",
        "            image.shape[1]\n",
        "            )      \n",
        "        \n",
        "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
        "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
        "    \n",
        "    return output_dict\n",
        "\n",
        "\n",
        "def show_inference(model, image_path, threshold = 0.5):\n",
        "    \"\"\"\n",
        "    Runs infernce on the given image at the image_path and also\n",
        "    draws the bounding box over the image .\n",
        "    \"\"\"\n",
        "    # the array based representation of the image will be used later in order to prepare the\n",
        "    # result image with boxes and labels on it.\n",
        "    print('Running inference for {}... '.format(image_path), end='')\n",
        "    image_np = np.array(Image.open(image_path))\n",
        "    # Actual detection.\n",
        "    output_dict = run_inference_for_single_image(model, image_np)\n",
        "    # Visualization of the results of a detection.\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np,\n",
        "        output_dict['detection_boxes'],\n",
        "        output_dict['detection_classes'],\n",
        "        output_dict['detection_scores'],\n",
        "        category_index,\n",
        "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
        "        use_normalized_coordinates=True,\n",
        "        max_boxes_to_draw=100,\n",
        "        min_score_thresh=threshold,\n",
        "        )\n",
        "    \n",
        "    print('Done !')\n",
        "    display(Image.fromarray(image_np))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioaovdftMn4o"
      },
      "source": [
        "Using the helper functions defined above let's run inference on Images :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dODEVBgPiUn4"
      },
      "source": [
        "#Get a random image from the test dataset\n",
        "path = TEST_IMAGE_PATHS[np.random.randint(0, len(TEST_IMAGE_PATHS))]\n",
        "#Run inference over the image and display the results\n",
        "show_inference(detection_model, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dwIyVfOiulx"
      },
      "source": [
        "#Get a random image from the test dataset\n",
        "path = TEST_IMAGE_PATHS[np.random.randint(0, len(TEST_IMAGE_PATHS))]\n",
        "#Run inference over the image and display the results\n",
        "show_inference(detection_model, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac1uuQSJjG34"
      },
      "source": [
        "#Get a random image from the test dataset\n",
        "path = TEST_IMAGE_PATHS[np.random.randint(0, len(TEST_IMAGE_PATHS))]\n",
        "#Run inference over the image and display the results\n",
        "show_inference(detection_model, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUJ-yeONRBt3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}