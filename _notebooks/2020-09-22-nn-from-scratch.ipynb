{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a NeuralNetwork from scratch\n",
    "> A tutorial to code a neural network from scratch in python using numpy.\n",
    "\n",
    "- toc: false\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [deeplearning python3.x numpy]\n",
    "- image: images/backprop.jpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will assume that you all know what a artificial neural network is and have a little bit of knowledge about `forward and backward propagation`. Just having a simple idea is enough.\n",
    "\n",
    "> Tip: If you do not know what the above terms are or would like to brush up on the topics, I would suggest going through this amazing [youtube playlist](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) by [3Blue1Brown](https://www.3blue1brown.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> youtube: https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "\n",
    "np.random.seed(123)\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this blog post, we'll use one of the most famous datasets in computer vision, [MNIST](https://en.wikipedia.org/wiki/MNIST_database). MNIST contains images of handwritten digits, collected by the National Institute of Standards and Technology and collated into a machine learning dataset by Yann Lecun and his colleagues. Lecun used MNIST in 1998 in [Lenet-5](http://yann.lecun.com/exdb/lenet/), the first computer system to demonstrate practically useful recognition of handwritten digit sequences. This was one of the most important breakthroughs in the history of AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code given below to download the `MNIST` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "wget -P path http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
    "```\n",
    "\n",
    "> Note: the above code snippet will download the dataset to `{path}` so be sure to set the `{path}` to the desired location of your choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (50000, 784)\n",
      "Total number of examples: 50000\n",
      "Number of pixel values per image: 784\n"
     ]
    }
   ],
   "source": [
    "def get_data(path):\n",
    "    \"\"\"\n",
    "    Fn to unzip the MNIST data and return\n",
    "    the data as numpy arrays\n",
    "    \"\"\"\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return map(np.array, (x_train,y_train,x_valid,y_valid))\n",
    "\n",
    "\n",
    "# Grab the MNIST dataset\n",
    "x_train,y_train,x_valid,y_valid = get_data(path= \"../../Datasets/mnist.pkl.gz\")\n",
    "\n",
    "tots,feats = x_train.shape\n",
    "print(\"Shape of x_train:\",x_train.shape)\n",
    "print(\"Total number of examples:\", tots)\n",
    "print(\"Number of pixel values per image:\", feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our `train` & `validation` datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our life a bit easier we are going to take only the examples that contain a 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10610, 784), (10610, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_mask = [y_train==0] # grab all the index values where 0 is present\n",
    "one_mask = [y_train==1] # grad all the index valus where 1 is present\n",
    "\n",
    "# grab all the 1's and 0's and make training set\n",
    "x_train = np.vstack((x_train[zero_mask], x_train[one_mask]))\n",
    "y_train = np.reshape(y_train, (-1,1))\n",
    "y_train = np.squeeze(np.vstack((y_train[zero_mask], y_train[one_mask]))).reshape(-1,1)\n",
    "\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our training set now has 10610 examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2055, 784), (2055, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_mask = [y_valid==0] # grab all the index values where 0 is present\n",
    "one_mask = [y_valid==1] # grad all the index valus where 1 is present\n",
    "\n",
    "# grab all the 1's and 0's and make training set\n",
    "x_valid = np.vstack((x_valid[zero_mask], x_valid[one_mask]))\n",
    "y_valid = np.reshape(y_valid, (-1,1))\n",
    "y_valid = np.squeeze(np.vstack((y_valid[zero_mask], y_valid[one_mask]))).reshape(-1,1)\n",
    "\n",
    "x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our validation set now has 2055 examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we need different training and validation sets ?**\n",
    "\n",
    "Since, this topic requires a different post on it's own I won't be covering it here. But you can get the idea from this above video:\n",
    "\n",
    "> youtube: https://youtu.be/1waHlpKiNyY?t=243"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view some example images from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#collapse \n",
    "plt.imshow(x_train[50].reshape(28,28), cmap=\"binary\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#collapse\n",
    "plt.imshow(x_train[5000].reshape(28,28), cmap=\"binary\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we are going to use a very basic model architecture this 2 linear layers and a output layer with 1 unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input \n",
    "import graphviz\n",
    "def gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"' + s + '; }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.44.0 (20200408.0750)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"811pt\" height=\"260pt\"\n",
       " viewBox=\"0.00 0.00 810.56 260.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-256 806.56,-256 806.56,4 -4,4\"/>\n",
       "<!-- X -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>X</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- linear1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>linear1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"125.1\" cy=\"-180\" rx=\"35.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"125.1\" y=\"-176.3\" font-family=\"Times,serif\" font-size=\"14.00\">linear1</text>\n",
       "</g>\n",
       "<!-- X&#45;&gt;linear1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>X&#45;&gt;linear1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M48.38,-222.55C60.94,-215.49 77.34,-206.28 91.66,-198.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"93.7,-201.1 100.7,-193.15 90.27,-194.99 93.7,-201.1\"/>\n",
       "</g>\n",
       "<!-- relu1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>relu1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"224.79\" cy=\"-180\" rx=\"28.7\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"224.79\" y=\"-176.3\" font-family=\"Times,serif\" font-size=\"14.00\">relu1</text>\n",
       "</g>\n",
       "<!-- linear1&#45;&gt;relu1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>linear1&#45;&gt;relu1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M160.27,-180C168.56,-180 177.47,-180 185.93,-180\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"185.93,-183.5 195.93,-180 185.93,-176.5 185.93,-183.5\"/>\n",
       "</g>\n",
       "<!-- linear2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>linear2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"324.49\" cy=\"-126\" rx=\"35.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"324.49\" y=\"-122.3\" font-family=\"Times,serif\" font-size=\"14.00\">linear2</text>\n",
       "</g>\n",
       "<!-- relu1&#45;&gt;linear2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>relu1&#45;&gt;linear2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M246.99,-168.28C259.79,-161.21 276.36,-152.05 290.81,-144.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"292.87,-146.92 299.93,-139.02 289.49,-140.79 292.87,-146.92\"/>\n",
       "</g>\n",
       "<!-- relu2 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>relu2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"424.18\" cy=\"-126\" rx=\"28.7\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"424.18\" y=\"-122.3\" font-family=\"Times,serif\" font-size=\"14.00\">relu2</text>\n",
       "</g>\n",
       "<!-- linear2&#45;&gt;relu2 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>linear2&#45;&gt;relu2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M359.65,-126C367.95,-126 376.86,-126 385.32,-126\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"385.32,-129.5 395.32,-126 385.32,-122.5 385.32,-129.5\"/>\n",
       "</g>\n",
       "<!-- linear3 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>linear3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"523.87\" cy=\"-72\" rx=\"35.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"523.87\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">linear3</text>\n",
       "</g>\n",
       "<!-- relu2&#45;&gt;linear3 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>relu2&#45;&gt;linear3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M446.38,-114.28C459.18,-107.21 475.75,-98.05 490.2,-90.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"492.26,-92.92 499.32,-85.02 488.88,-86.79 492.26,-92.92\"/>\n",
       "</g>\n",
       "<!-- sigmoid -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>sigmoid</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"634.62\" cy=\"-72\" rx=\"39.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"634.62\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">sigmoid</text>\n",
       "</g>\n",
       "<!-- linear3&#45;&gt;sigmoid -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>linear3&#45;&gt;sigmoid</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M558.98,-72C567.13,-72 575.99,-72 584.66,-72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"584.7,-75.5 594.7,-72 584.7,-68.5 584.7,-75.5\"/>\n",
       "</g>\n",
       "<!-- prediction -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>prediction</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"756.41\" cy=\"-72\" rx=\"46.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"756.41\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">prediction</text>\n",
       "</g>\n",
       "<!-- sigmoid&#45;&gt;prediction -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>sigmoid&#45;&gt;prediction</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M674.51,-72C682.66,-72 691.41,-72 700.03,-72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"700.05,-75.5 710.05,-72 700.05,-68.5 700.05,-75.5\"/>\n",
       "</g>\n",
       "<!-- W1 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>W1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-180\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-176.3\" font-family=\"Times,serif\" font-size=\"14.00\">W1</text>\n",
       "</g>\n",
       "<!-- W1&#45;&gt;linear1 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>W1&#45;&gt;linear1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.01,-180C61.92,-180 70.84,-180 79.61,-180\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"79.76,-183.5 89.76,-180 79.76,-176.5 79.76,-183.5\"/>\n",
       "</g>\n",
       "<!-- B1 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>B1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-126\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-122.3\" font-family=\"Times,serif\" font-size=\"14.00\">B1</text>\n",
       "</g>\n",
       "<!-- B1&#45;&gt;linear1 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>B1&#45;&gt;linear1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M48.38,-137.45C60.94,-144.51 77.34,-153.72 91.66,-161.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"90.27,-165.01 100.7,-166.85 93.7,-158.9 90.27,-165.01\"/>\n",
       "</g>\n",
       "<!-- W2 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>W2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"224.79\" cy=\"-126\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"224.79\" y=\"-122.3\" font-family=\"Times,serif\" font-size=\"14.00\">W2</text>\n",
       "</g>\n",
       "<!-- W2&#45;&gt;linear2 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>W2&#45;&gt;linear2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M251.97,-126C260.27,-126 269.68,-126 278.89,-126\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"279.14,-129.5 289.14,-126 279.14,-122.5 279.14,-129.5\"/>\n",
       "</g>\n",
       "<!-- B2 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>B2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"224.79\" cy=\"-72\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"224.79\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">B2</text>\n",
       "</g>\n",
       "<!-- B2&#45;&gt;linear2 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>B2&#45;&gt;linear2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M246.03,-83.19C258.99,-90.35 276.1,-99.81 290.94,-108.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"289.38,-111.15 299.83,-112.92 292.77,-105.02 289.38,-111.15\"/>\n",
       "</g>\n",
       "<!-- W3 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>W3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"424.18\" cy=\"-72\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"424.18\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">W3</text>\n",
       "</g>\n",
       "<!-- W3&#45;&gt;linear3 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>W3&#45;&gt;linear3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M451.36,-72C459.66,-72 469.07,-72 478.28,-72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"478.53,-75.5 488.53,-72 478.53,-68.5 478.53,-75.5\"/>\n",
       "</g>\n",
       "<!-- B3 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>B3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"424.18\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"424.18\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">B3</text>\n",
       "</g>\n",
       "<!-- B3&#45;&gt;linear3 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>B3&#45;&gt;linear3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M445.42,-29.19C458.38,-36.35 475.49,-45.81 490.33,-54.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"488.77,-57.15 499.22,-58.92 492.16,-51.02 488.77,-57.15\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7fbfbb3d4810>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "gv('''\n",
    "X->linear1->relu1->linear2->relu2->linear3->sigmoid->prediction\n",
    "\n",
    "W1->linear1\n",
    "B1->linear1\n",
    "\n",
    "W2->linear2\n",
    "B2->linear2\n",
    "\n",
    "W3->linear3\n",
    "B3->linear3\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then using the `predicted` value from the above model and the `real_target` value the loss is calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.44.0 (20200408.0750)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"190pt\" height=\"98pt\"\n",
       " viewBox=\"0.00 0.00 190.29 98.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 94)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-94 186.29,-94 186.29,4 -4,4\"/>\n",
       "<!-- prediction -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>prediction</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"46.15\" cy=\"-72\" rx=\"46.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"46.15\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">prediction</text>\n",
       "</g>\n",
       "<!-- loss -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>loss</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"155.29\" cy=\"-45\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"155.29\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">loss</text>\n",
       "</g>\n",
       "<!-- prediction&#45;&gt;loss -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>prediction&#45;&gt;loss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M85.22,-62.4C96.55,-59.55 108.89,-56.44 119.97,-53.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"120.88,-57.03 129.72,-51.19 119.16,-50.24 120.88,-57.03\"/>\n",
       "</g>\n",
       "<!-- target -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>target</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"46.15\" cy=\"-18\" rx=\"30.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"46.15\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">target</text>\n",
       "</g>\n",
       "<!-- target&#45;&gt;loss -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>target&#45;&gt;loss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M74.46,-24.88C88.26,-28.36 105.16,-32.62 119.84,-36.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"119.04,-39.73 129.59,-38.78 120.75,-32.94 119.04,-39.73\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7fbf83a01290>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "gv('''\n",
    "prediction->loss\n",
    "target->loss\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a deep dive into what this network means:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take at look at all the individual components of this network:\n",
    "- **Linear:**\n",
    "  The linear layer computes the following :      \n",
    "   ```\n",
    "   out = matmul(input,W1) + B1\n",
    "   ```\n",
    "   \n",
    "- **ReLU:** \n",
    "  The relu computes the following:\n",
    "  ```\n",
    "  out = max(0, input)\n",
    "  ```\n",
    "- **Sigmoid:** \n",
    "  The sigmoid computes the following:\n",
    "  ```\n",
    "  out = 1/(1 + e.pow(input))\n",
    "  ```\n",
    "  \n",
    "- **Loss:** \n",
    "  For the loss we are going to use the CrossEntropy Loss which is defined by the follwoing equation:\n",
    "  $$loss= -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(yhat^{(i)}\\right) + (1-y^{(i)})\\log\\left(1-yhat^{(i)}\\right)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have our model architecture, let's create the different parts needed to assemble the model:**\n",
    "- linear layer\n",
    "- relu activation\n",
    "- sigmoid activation\n",
    "- loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's first try to make some sense of what is happening in the backward and forward pass of our model:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On paper our forward pass would look something like this:**\n",
    "\n",
    "> Note: `@` in python is the `matrix-multiplication operator`. \n",
    "\n",
    "```python\n",
    "inputs = x # original inputs\n",
    "targets = y # original targets\n",
    "\n",
    "z1 = inputs @ w2 + b2\n",
    "a1 = relu(z1)\n",
    "\n",
    "z2 = a1 @ w2 + b2\n",
    "a2 = sigmoid(z2) # this is our model prediction\n",
    "\n",
    "loss = loss_fn(a2, targets)\n",
    "```\n",
    "> Note: This is not actual code it's just psuedo-code for understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consequently our backward pass would look something like this :** \n",
    "\n",
    "(Let us assume that the `grad(inp, out)` computes the gradients of `inp` wrt `out`)\n",
    "\n",
    "```python\n",
    "# gradient of loss wrt output of the previous activation layer\n",
    "da2 = grad(loss, a2) \n",
    "\n",
    "# gradient of loss wrt to z2\n",
    "dz2 = grad(loss, z2) = grad(loss, a2) * grad(a2,z2) \n",
    "\n",
    "# gradient of the loss wrt to weight w2: [current layer]\n",
    "dw2 = grad(loss, w2) = grad(loss, z2) * grad(z2, w2) = dz2 * grad(z2, w2)\n",
    "# gradient of the loss wrt to bias b2: [current layer]\n",
    "db2 = grad(loss, b2) = grad(loss, z2) * grad(z2, b2) = dz2 * grad(z2, b2)\n",
    "# gradient of loss wrt a1: [previous layer]\n",
    "da1 = grad(loss, a1) = grad(loss, z2) * grad(z2, a1) = dz2 * grad(z2, a1)\n",
    "\n",
    "# gradient of loss wrt z1\n",
    "dz1 = grad(loss, z1) = grad(loss, a1) * grad(a1, z1) = da1 * grad(a1, z1)\n",
    "\n",
    "# gradient of the loss wrt to weight w1: [current layer]\n",
    "dw1 = grad(loss, w1) = grad(loss, z1) * grad(z1, w1) = dz1 * grad(z1, w1)\n",
    "# gradient of the loss wrt to bias b1: [current layer]\n",
    "db1 = grad(loss, b1) = grad(loss, z1) * grad(z1, b1) = dz1 * grad(z1, b1)\n",
    "# gradient of the loss wrt to a0: [previous layer]\n",
    "da0 = grad(loss, a0) = grad(loss, z1) * grad(z1, a0) = dz2 * grad(z1, a0)\n",
    "\n",
    "\n",
    "# Update parameters :\n",
    "# since we now have all the required gradients we can now perform the update step\n",
    "w1 -= learning_rate * dw1\n",
    "b1 -= learning_rate * db1\n",
    "\n",
    "w2 -= learning_rate * dw2\n",
    "b2 -= learning_rate * db2\n",
    "```\n",
    "> Note: This is not actual code it's just psuedo-code for understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `linear` layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code creates a `Linear class` which represents a `Linear` layer in our neural-network. The `forward function` of the class implements the of the `layer's forward propagation` & the `backward function` implements the `layers's backward propagation`. Let's go to detail into what the code means:\n",
    "\n",
    "- **Forward:**  \n",
    "This part is quite straight-forward it computes the dot-product between the **`input`** and the **`weights`** & adds the **`bias`** term to get **`z`**. It also stores all the intermidiate values generated to use in the backward pass.\n",
    "\n",
    "\n",
    "- **Backward:**\n",
    "    * The backward method of the class **`Linear`** takes in the argument **`grads`**. \n",
    "    * **`grads`** is the gradient of the loss wrt to the output of the current linear layer ie., **`dz`** if we were to follow the nomenclature of our pseudo-code.\n",
    "    * To succesfully compute the backward pass for our linear layer we need the following:\n",
    "        - **`grad(z, w)`** \n",
    "        - **`grad(z, b)`**\n",
    "        - **`grad(z, a_prev)`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> Note: `z`, `w`, `b`, `a_prev` are the outputs, weights, bias and input-activations of the Linear layer respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \"\"\"\n",
    "        Implement the linear part of a layer's forward propagation.\n",
    "        \n",
    "        Args:\n",
    "            inp : activations from previous layer (or input data)\n",
    "        \n",
    "        Returns:\n",
    "\n",
    "            z  : the input of the activation function, also called pre-activation parameter \n",
    "        \"\"\"\n",
    "        self.inp = inp\n",
    "        self.z   = inp @ self.w + self.b\n",
    "        return self.z\n",
    "    \n",
    "    def backward(self, grads):\n",
    "        \"\"\"\n",
    "        Implement the linear portion of backward propagation for a single layer.\n",
    "\n",
    "        Args:\n",
    "            grads :  Gradient of the cost with respect to the linear output. \n",
    "                     or the accumulated gradients from the prev layers. \n",
    "                     This is used for the chain rule to compute the gradients.\n",
    "        Returns:\n",
    "            da : Gradient of cost wrt to the activation of the previous layer or the input of the \n",
    "                 current layer.\n",
    "            dw : Gradient of the cost with respect to W\n",
    "            db : Gradient of the cost with respect to b\n",
    "        \"\"\"\n",
    "        m = self.inp.shape[1]\n",
    "        # gradient of loss wrt to the weights\n",
    "        dw = 1/m * (self.inp.T @ grads)\n",
    "        # gradient of the loss wrt to the bias\n",
    "        db = 1/m * np.sum(grads, axis=0, keepdims=True)\n",
    "        # gradient of the loss wrt to the input of the linear layer\n",
    "        # this is used to continue the chain rule\n",
    "        da_prev = grads @ self.w.T \n",
    "        return (da_prev, dw, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ReLU` layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Forward**:  \n",
    "The mathematical formula for ReLU is $A = RELU(Z) = max(0, Z)$\n",
    "- **Backward**:  \n",
    "During the backward pass the relu accepts the gradients of the `loss wrt to the activation` i.e, `da` then computes\n",
    "the gradients of the `loss wrt to the input-of-relu(z)` i.e, `dz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelU:\n",
    "    def forward(self, inp):\n",
    "        \"\"\"\n",
    "        Implement the RELU function.\n",
    "\n",
    "        Args:\n",
    "            inp : Output of the linear layer, of any shape\n",
    "\n",
    "        Returns:\n",
    "            a  : Post-activation parameter, of the same shape as Z\n",
    "        \"\"\"\n",
    "        self.inp = inp\n",
    "        self.output = np.maximum(0, self.inp)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grads):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "        Ars:\n",
    "            grads : gradients of the loss wrt to the activation output\n",
    "\n",
    "        Returns:\n",
    "            dz : Gradient of the loss with respect to the input of the activation\n",
    "        \"\"\"\n",
    "        dz = np.array(grads, copy=True)\n",
    "        dz[self.inp <= 0] = 0\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sigmoid` layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid layer functions in exactly the same way as the `ReLU` layer . The only difference is the forward pass output calculation. \n",
    "\n",
    "\n",
    "In the `sigmoid layer`:  $\\sigma(Z) = \\frac{1}{ 1 + e^{-(W A + b)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self, inp):\n",
    "        \"\"\"\n",
    "        Implements the sigmoid activation in numpy\n",
    "\n",
    "        Args:\n",
    "            inp: numpy array of any shape\n",
    "\n",
    "        Returns:\n",
    "            a  : output of sigmoid(z), same shape as inp\n",
    "        \"\"\"\n",
    "        self.inp = inp\n",
    "        self.out =  1/(1+np.exp(-self.inp))\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, grads):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a single sigmoid unit.\n",
    "\n",
    "        Args:\n",
    "            grads : gradients of the loss wrt to the activation output\n",
    "\n",
    "        Returns:\n",
    "            dz : Gradient of the loss with respect to the input of the activation\n",
    "        \"\"\"\n",
    "        s = 1/(1+np.exp(-self.inp))\n",
    "        dz = grads * s * (1-s)\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss_function :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we are going to use the [CrossEntropy Loss](https://en.wikipedia.org/wiki/Cross_entropy)\n",
    "\n",
    "The `forward` pass of the CrossEntropy Loss is computed as follows: \n",
    "$$loss= -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(yhat^{(i)}\\right) + (1-y^{(i)})\\log\\left(1-yhat^{(i)}\\right)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CELoss():\n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        Implement the CrossEntropy loss function.\n",
    "\n",
    "        Args:\n",
    "            pred   : predicted labels from the neural network\n",
    "            target : true \"label\" labels\n",
    "        Returns:\n",
    "            loss : cross-entropy loss\n",
    "        \"\"\"\n",
    "        self.yhat = pred\n",
    "        self.y = target\n",
    "        m = self.y.shape[0]\n",
    "        # commpute loss\n",
    "        term1 = (np.multiply(self.y, np.log(self.yhat)))\n",
    "        term2 = (np.multiply((1-self.y),(np.log(1-self.yhat))))\n",
    "        loss = -1/m * np.sum(term1+term2)\n",
    "        self.output = loss\n",
    "        return np.squeeze(self.output) # convert array to a single value number\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Computes the gradinets of the loss_fn wrt to the predicted labels\n",
    "        \n",
    "        Returns:\n",
    "         da : derivative of loss_fn wrt to the predicted labels\n",
    "        \"\"\"\n",
    "        # derivative of loss_fn with respect to a [predicted labels]\n",
    "        da = - (np.divide(self.y, self.yhat) - np.divide(1 - self.y, 1 - self.yhat)) \n",
    "        return da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model:\n",
    "\n",
    "**Let's go over the architecture that we are going to use for our neural netwok:**\n",
    "\n",
    "Our model is going to have 2 hidden layers and a output layer. The `hidden layers` are going to have `16 units` each followed by a `ReLU` activation layer and the `output layer` is going to have `1 unit` followed by a `Sigmoid` unit. The ouput layer is going to predict the `input` is either `0` or `1`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assemble the layers required to construct out model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10610, 784), (10610, 1))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are our inputs and targets\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 16), (1, 16), (16, 16), (1, 16), (16, 1), (1, 1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nh1 = 16 # no. of units in the first hidden layer\n",
    "nh2 = 16 # no. of units in the 2nd hidden layer\n",
    "nh3 = 1 # no. of units in the output layer\n",
    "\n",
    "w1  = np.random.randn(x_train.shape[1], nh1) * 0.01\n",
    "b1  = np.zeros((1, nh1))\n",
    "\n",
    "w2  = np.random.randn(nh1, nh2) * 0.01\n",
    "b2  = np.zeros((1, nh2))\n",
    "\n",
    "w3  = np.random.randn(nh2, nh3)\n",
    "b3  = np.zeros((1, nh3))\n",
    "\n",
    "w1.shape, b1.shape, w2.shape, b2.shape, w3.shape, b3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instaniating the layers needed to construct our model\n",
    "\n",
    "lin1 = Linear(w1,b1) # 1 hidden layer\n",
    "relu1 = RelU()\n",
    "lin2 = Linear(w2,b2) # 2nd hidden layer\n",
    "relu2 = RelU()\n",
    "lin3 = Linear(w3,b3) # output layer\n",
    "sigmoid = Sigmoid()\n",
    "\n",
    "loss_fn = CELoss() # loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6954586218304929\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "z1 = lin1.forward(x_train)\n",
    "a1 = relu1.forward(z1)\n",
    "z2 = lin2.forward(a1)\n",
    "a2 = relu2.forward(z2)\n",
    "z3 = lin3.forward(a2)\n",
    "pred = sigmoid.forward(z3)\n",
    "\n",
    "# calculate loss\n",
    "loss = loss_fn.forward(pred, y_train)\n",
    "print(\"Loss:\", loss) # print out the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [[0.50221048]\n",
      " [0.5016568 ]\n",
      " [0.50445   ]\n",
      " ...\n",
      " [0.5006608 ]\n",
      " [0.49939964]\n",
      " [0.49894181]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions: \", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "da3 = sigmoid.backward(loss_fn.backward())\n",
    "dz3, dw3, db3 = lin3.backward(da3)\n",
    "\n",
    "da2 = relu2.backward(dz3)\n",
    "dz2, dw2, db2 = lin2.backward(da2)\n",
    "\n",
    "da1 = relu1.backward(dz2)\n",
    "_, dw1, db1 = lin1.backward(da1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set learning rate\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# update parameters \n",
    "lin1.w -= learning_rate * dw1\n",
    "lin2.w -= learning_rate * dw2\n",
    "lin3.w -= learning_rate * dw3\n",
    "\n",
    "lin1.b -= learning_rate * db1\n",
    "lin2.b -= learning_rate * db2\n",
    "lin3.b -= learning_rate * db3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 16) (1, 16) (16, 16) (1, 16) (16, 1) (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate parameters\n",
    "\n",
    "nh1 = 16 # no. of units in the first hidden layer\n",
    "nh2 = 16 # no. of units in the 2nd hidden layer\n",
    "nh3 = 1 # no. of units in the output layer\n",
    "\n",
    "w1  = np.random.randn(x_train.shape[1], nh1) * 0.01\n",
    "b1  = np.zeros((1, nh1))\n",
    "\n",
    "w2  = np.random.randn(nh1, nh2) * 0.01\n",
    "b2  = np.zeros((1, nh2))\n",
    "\n",
    "w3  = np.random.randn(nh2, nh3)\n",
    "b3  = np.zeros((1, nh3))\n",
    "\n",
    "print(w1.shape, b1.shape, w2.shape, b2.shape, w3.shape, b3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, learning_rate):\n",
    "        \"\"\"\n",
    "        A simple neural network model\n",
    "        The `forward` method computes the forward propagation step of the model\n",
    "        The `backward` method computes the backward step propagation of the model\n",
    "        The `update_step` method updates the parameters of the model\n",
    "        \"\"\"\n",
    "        self.lin1    = Linear(w1,b1)\n",
    "        self.relu1   = RelU()\n",
    "        self.lin2    = Linear(w2,b2)\n",
    "        self.relu2   = RelU()\n",
    "        self.lin3    = Linear(w3,b3)\n",
    "        self.sigmoid = Sigmoid()\n",
    "        self.loss_fn = CELoss()\n",
    "        self.lr      = learning_rate\n",
    "        self.losses  = [] # stores the loss at each iteration\n",
    "\n",
    "\n",
    "    def forward(self, inp, calc_loss=True):\n",
    "        \"\"\"\n",
    "        Computs the forward step for out model\n",
    "        Returns the loss and the prediction of the model\n",
    "        \"\"\"\n",
    "        out  = self.relu1.forward(self.lin1.forward(inp))\n",
    "        out  = self.relu2.forward(self.lin2.forward(out))\n",
    "        pred = self.sigmoid.forward(self.lin3.forward(out))\n",
    "        \n",
    "        if calc_loss:\n",
    "            loss = self.loss_fn.forward(pred, y_train)\n",
    "            # appending the loss of the current iteration\n",
    "            self.losses.append(loss)\n",
    "            return loss, pred\n",
    "        else:\n",
    "            return pred\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Computes the backward step\n",
    "        and return the gradients of the parameters with the loss\n",
    "        \"\"\"\n",
    "        da3 = self.sigmoid.backward(self.loss_fn.backward())\n",
    "        dz3, dw3, db3 = self.lin3.backward(da3)\n",
    "\n",
    "        da2 = self.relu2.backward(dz3)\n",
    "        dz2, dw2, db2 = self.lin2.backward(da2)\n",
    "\n",
    "        da1 = self.relu1.backward(dz2)\n",
    "        _, dw1, db1 = self.lin1.backward(da1)\n",
    "\n",
    "        self.dws = [dw1, dw2, dw3]\n",
    "        self.dbs = [db1, db2, db3]\n",
    "\n",
    "\n",
    "    def update_step(self):\n",
    "        \"\"\"\n",
    "        Performs the update step\n",
    "        \"\"\"\n",
    "        self.lin1.w -= self.lr * self.dws[0]\n",
    "        self.lin2.w -= self.lr * self.dws[1]\n",
    "        self.lin3.w -= self.lr * self.dws[2]\n",
    "\n",
    "        self.lin1.b -= self.lr * self.dbs[0]\n",
    "        self.lin2.b -= self.lr * self.dbs[1]\n",
    "        self.lin3.b -= self.lr * self.dbs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after interation 0 is 0.6935\n",
      "Loss after interation 1 is 0.6829\n",
      "Loss after interation 2 is 0.6675\n",
      "Loss after interation 3 is 0.6481\n",
      "Loss after interation 4 is 0.6228\n",
      "Loss after interation 5 is 0.5924\n",
      "Loss after interation 6 is 0.5488\n",
      "Loss after interation 7 is 0.5149\n",
      "Loss after interation 8 is 0.4527\n",
      "Loss after interation 9 is 0.3746\n",
      "Loss after interation 10 is 0.3095\n",
      "Loss after interation 11 is 0.2433\n",
      "Loss after interation 12 is 0.2016\n",
      "Loss after interation 13 is 0.1698\n",
      "Loss after interation 14 is 0.1448\n",
      "Loss after interation 15 is 0.1250\n",
      "Loss after interation 16 is 0.1091\n",
      "Loss after interation 17 is 0.0963\n",
      "Loss after interation 18 is 0.0859\n",
      "Loss after interation 19 is 0.0773\n",
      "Loss after interation 20 is 0.0702\n",
      "Loss after interation 21 is 0.0641\n",
      "Loss after interation 22 is 0.0590\n",
      "Loss after interation 23 is 0.0546\n",
      "Loss after interation 24 is 0.0509\n",
      "Loss after interation 25 is 0.0476\n",
      "Loss after interation 26 is 0.0447\n",
      "Loss after interation 27 is 0.0421\n",
      "Loss after interation 28 is 0.0399\n",
      "Loss after interation 29 is 0.0378\n",
      "Loss after interation 30 is 0.0360\n",
      "Loss after interation 31 is 0.0344\n",
      "Loss after interation 32 is 0.0329\n",
      "Loss after interation 33 is 0.0316\n",
      "Loss after interation 34 is 0.0304\n",
      "Loss after interation 35 is 0.0292\n",
      "Loss after interation 36 is 0.0282\n",
      "Loss after interation 37 is 0.0273\n",
      "Loss after interation 38 is 0.0264\n",
      "Loss after interation 39 is 0.0256\n",
      "Loss after interation 40 is 0.0248\n",
      "Loss after interation 41 is 0.0241\n",
      "Loss after interation 42 is 0.0234\n",
      "Loss after interation 43 is 0.0228\n",
      "Loss after interation 44 is 0.0222\n",
      "Loss after interation 45 is 0.0217\n",
      "Loss after interation 46 is 0.0212\n",
      "Loss after interation 47 is 0.0207\n",
      "Loss after interation 48 is 0.0202\n",
      "Loss after interation 49 is 0.0198\n"
     ]
    }
   ],
   "source": [
    "nn = Model(learning_rate=0.0005)\n",
    "epochs = 50 # no. of iterations to train\n",
    "\n",
    "for n in range(epochs):\n",
    "    loss, _ = nn.forward(x_train)\n",
    "    nn.backward()\n",
    "    nn.update_step()\n",
    "    print(f\"Loss after interation {n} is {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#collapse\n",
    "plt.plot(nn.losses, color=\"salmon\")\n",
    "plt.title(\"Loss per Iteration\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing accuracy of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's check our model performance by computing the `accuracy` on the `validation` dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_accuracy(preds, targs):\n",
    "    \"\"\"\n",
    "    Fn that computes the accuracy between the predicted values and the targets\n",
    "    \"\"\"\n",
    "    m = len(targs)\n",
    "    p = np.zeros_like(preds)\n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] > 0.5:\n",
    "            p[i] = 1\n",
    "        else:\n",
    "            p[i] = 0\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == targs)/m)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9965936739659367\n"
     ]
    }
   ],
   "source": [
    "# generate predicitons from out model\n",
    "preds = nn.forward(x_valid, calc_loss=False)\n",
    "# compute accuracy\n",
    "comp_accuracy(preds, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Nice our model achieves a `accuracy` of **`0.9965936739659367`** on the validation set !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting predictions from our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQWUlEQVR4nO3df6xU5Z3H8c9nFZZUJOhyRYJWug2GGGPRjLDJYuOmpSpb0Yqw9dfa2EiTVddGEyEsG4UYJf6gUbNpREWorbZVJJrVxbpmG/UfZSQIqGl13YuF8OOyVEQ3atXv/nGH5qJ3nrnMOfPj+rxfyc2ce75z5nwZ78czM8+c8zgiBODL7y863QCA9iDsQCYIO5AJwg5kgrADmSDsQCYIO5AJwv4lZLvX9rfbsJ+bbP+81ftBOQg7kAnC/iVn+we2X7R9h+0/2v4f2+cMqP/W9q22X7b9nu0nbB9dq51pe9vnHq/X9rdtny1pkaR/sP2+7Vfb+y/DoSLseZgu6XeSxkm6TdIDtj2g/o+SrpA0QdInku5u9IARsU7SLZJ+FRGjI+IbkmR7oe1/L7l/lICw52FrRNwXEZ9KWq3+UI8fUH8oIrZExAeS/lXSPNuHNbOjiFgWEd8t3jLKRtjzsPPAQkT8X21x9ID6HwYsb5U0Qv2vAvAlQtghSccPWP6qpD9J2iPpA0lfOVCoHe17BtyXUyaHEcIOSbrU9km2vyJpqaTHai/5fy9plO2/tz1C0mJJfzlgu12SJtnm72gY4D8SJOkhSavU/3J/lKR/lqSI2CfpnyTdL2m7+o/0Az+df7R2+7+2N0iS7UW2/6M9beNQmItX5M32byX9PCLu73QvaC2O7EAmCDuQCV7GA5ngyA5k4vB27mzcuHExadKkdu4SyEpvb6/27NnjwWqFwl47GeIuSYdJuj8ilqXuP2nSJFWr1SK7BJBQqVTq1pp+GV/7NtW/STpH0kmSLrJ9UrOPB6C1irxnnybprYh4OyI+lvRLSeeV0xaAshUJ+0QdfALFttq6g9ieb7tqu9rX11dgdwCKaPmn8RGxIiIqEVHp6elpvAGAligS9u06+Gyp42rrAHShImFfL2my7a/ZHinp+5KeLKctAGVreugtIj6xfbWkZ9Q/9LYyIl4rrTMApSo0zh4RT0t6uqReALQQX5cFMkHYgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMtHWKZsx/KxZsyZZX7RoUbL+zjvv1K3ddtttyW2vueaaZB2HhiM7kAnCDmSCsAOZIOxAJgg7kAnCDmSCsAOZYJy9C3z44YfJ+ltvvZWsp8ar169fn9z24osvTtZXrlyZrPf29ibrtuvWFi9enNx2ypQpyfrMmTOTdRysUNht90raL+lTSZ9ERKWMpgCUr4wj+99FxJ4SHgdAC/GeHchE0bCHpN/YfsX2/MHuYHu+7artal9fX8HdAWhW0bDPiIjTJJ0j6Srb3/z8HSJiRURUIqLS09NTcHcAmlUo7BGxvXa7W9JaSdPKaApA+ZoOu+0jbB95YFnSdyRtKasxAOUq8mn8eElra+Ooh0t6OCLWldJVZpYtW5asL1myJFlPjWU3cuONNza9bVH79+9P1i+88MJk/Y477kjWr7zyykPu6cus6bBHxNuSvlFiLwBaiKE3IBOEHcgEYQcyQdiBTBB2IBOOiLbtrFKpRLVabdv+ukWjf/OMGTOS9cMPTw+aXHfddXVrRx55ZHLbpUuXJusffPBBsj537txkfcSIEXVrDz/8cHLbovbu3Vu3Nnbs2Jbuu1MqlYqq1eqgY7Ec2YFMEHYgE4QdyARhBzJB2IFMEHYgE4QdyASXkm6Dxx57LFn/+OOPk/VGl3tOjZVv3bo1ue3NN9+crDfSaFrl6dOn162dcsopyW0XLlzYVE8HXHHFFXVrjz/+eKHHHo44sgOZIOxAJgg7kAnCDmSCsAOZIOxAJgg7kAnG2dug0TUDGtWLjDffcsstyfp7772XrDe6zHWjc/FTbrjhhmT97bffTtbvvffeZH3t2rV1ay+88EJy2zPOOCNZH444sgOZIOxAJgg7kAnCDmSCsAOZIOxAJgg7kAnG2Uvw0UcfJeubNm1K1htNuXziiSceck8HfPjhh4X2vW3btqb3XdTdd9+drK9fvz5Z37BhQ91ao/PZsxxnt73S9m7bWwasO9r2s7bfrN0e1do2ARQ1lJfxqySd/bl1CyU9FxGTJT1X+x1AF2sY9oh4XtLn59E5T9Lq2vJqSeeX3BeAkjX7Ad34iNhRW94paXy9O9qeb7tqu9rX19fk7gAUVfjT+Og/i6PumRwRsSIiKhFR6enpKbo7AE1qNuy7bE+QpNrt7vJaAtAKzYb9SUmX15Yvl/REOe0AaJWG4+y2H5F0pqRxtrdJulHSMkm/tv1DSVslzWtlk91u3bp1yfozzzyTrM+cObPMdg7S6Hz1MWPGJOtXXXVVme0ckpEjRybr5557brKeGmdftWpVctsFCxYk68cee2yy3o0ahj0iLqpT+lbJvQBoIb4uC2SCsAOZIOxAJgg7kAnCDmSCU1xL8MorrxTa/uSTTy60fWpor9Gw4NSpU5P1KVOmNNVTOzS6xPZdd91Vt7Zv377kti+//HKyPnv27GS9G3FkBzJB2IFMEHYgE4QdyARhBzJB2IFMEHYgE4yzl2Dz5s2Ftp8zZ06h7W+99da6tUaXub7gggsK7buTRo0alayn/m0PPvhgcts333yzqZ66GUd2IBOEHcgEYQcyQdiBTBB2IBOEHcgEYQcywTh7CfonxWm+XtSuXbs6tu/hqtHz8uKLLybr119/fZnttAVHdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMsE4ewlsF6oXdckll9St3XPPPclt586dW3Y7w0Kn/5t1QsMju+2Vtnfb3jJg3U22t9veWPuZ1do2ARQ1lJfxqySdPcj6n0TE1NrP0+W2BaBsDcMeEc9L2tuGXgC0UJEP6K62van2Mv+oeneyPd921Xa1r6+vwO4AFNFs2H8q6euSpkraIenOeneMiBURUYmISk9PT5O7A1BUU2GPiF0R8WlEfCbpPknTym0LQNmaCrvtCQN+/Z6kLfXuC6A7NBxnt/2IpDMljbO9TdKNks60PVVSSOqV9KMW9jjsjR07Nlk/5phjCj3+4sWLm6ohLw3DHhEXDbL6gRb0AqCF+LoskAnCDmSCsAOZIOxAJgg7kAlOcW2Dd999N1l/6qmnkvVrr722zHayUa1Wm972tNNOK7GT7sCRHcgEYQcyQdiBTBB2IBOEHcgEYQcyQdiBTDDOXoJZs9IX133iiSeS9WXLliXrjLMPbvny5cn6pk2b6tYaXSr6lFNOaaqnbsaRHcgEYQcyQdiBTBB2IBOEHcgEYQcyQdiBTDDOXoJGl4KOiGR9586dyfqaNWuS9Tlz5iTrw9Wrr76arN9+++1NP/a4ceOS9dmzZzf92N2KIzuQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5kYypTNx0v6maTx6p+ieUVE3GX7aEm/kjRJ/dM2z4uIP7au1e41efLkZH3ChAnJeqNx9ssuuyxZf/311+vWFixYkNx25MiRyXpRn332Wd3axo0bk9s2GuvetWtXUz1J0qWXXtr0tsPVUI7sn0i6PiJOkvQ3kq6yfZKkhZKei4jJkp6r/Q6gSzUMe0TsiIgNteX9kt6QNFHSeZJW1+62WtL5rWoSQHGH9J7d9iRJp0p6SdL4iNhRK+1U/8t8AF1qyGG3PVrSGkk/joj3Btai/8vfg34B3PZ821Xb1b6+vkLNAmjekMJue4T6g/6LiHi8tnqX7Qm1+gRJuwfbNiJWREQlIio9PT1l9AygCQ3D7v7LcD4g6Y2IGHg5zyclXV5bvlxS+hKqADrKjU6/tD1D0guSNks6MI6ySP3v238t6auStqp/6G1v6rEqlUoUmUZ3uNq2bVuyfuqppybre/bsSdZTl0WePn16cttjjz02WW80rNjIli1b6tbWrVtX6LEbOeuss+rWHn300eS2o0ePLrudtqhUKqpWq4P+QTQcZ4+IFyXV+2v6VpHGALQP36ADMkHYgUwQdiAThB3IBGEHMkHYgUxwKek2OO6445L1p59+OlmfN29esr5169a6tZdeeim5bVFD+J5G0489fnz6dItGp/4uXbq0bm3UqFFN9TSccWQHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiATjLN3gdNPPz1ZbzR18ZIlS+rWUueTS9K+ffuS9dQYvtT4MtjTpk2rW1u+fHndmiSdcMIJyfrEiROTdRyMIzuQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5lgnH0YGDNmTLJ+5513tqkTDGcc2YFMEHYgE4QdyARhBzJB2IFMEHYgE4QdyETDsNs+3vZ/2X7d9mu2r62tv8n2dtsbaz+zWt8ugGYN5Us1n0i6PiI22D5S0iu2n63VfhIRd7SuPQBlaRj2iNghaUdteb/tNyRxiRBgmDmk9+y2J0k6VdKBOYWutr3J9krbR9XZZr7tqu1qX19foWYBNG/IYbc9WtIaST+OiPck/VTS1yVNVf+Rf9AvaEfEioioRESlp6enhJYBNGNIYbc9Qv1B/0VEPC5JEbErIj6NiM8k3Sep/pUFAXTcUD6Nt6QHJL0REcsHrJ8w4G7fk5S+jCmAjhrKp/F/K+kySZttb6ytWyTpIttTJYWkXkk/akmHAEoxlE/jX5Q02CTb6UnFAXQVvkEHZIKwA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5lwRLRvZ3afpK0DVo2TtKdtDRyabu2tW/uS6K1ZZfZ2QkQMev23tob9Czu3qxFR6VgDCd3aW7f2JdFbs9rVGy/jgUwQdiATnQ77ig7vP6Vbe+vWviR6a1Zbeuvoe3YA7dPpIzuANiHsQCY6EnbbZ9v+ne23bC/sRA/12O61vbk2DXW1w72stL3b9pYB6462/aztN2u3g86x16HeumIa78Q04x197jo9/Xnb37PbPkzS7yXNlLRN0npJF0XE621tpA7bvZIqEdHxL2DY/qak9yX9LCJOrq27TdLeiFhW+x/lURGxoEt6u0nS+52exrs2W9GEgdOMSzpf0g/Uwecu0dc8teF568SRfZqktyLi7Yj4WNIvJZ3XgT66XkQ8L2nv51afJ2l1bXm1+v9Y2q5Ob10hInZExIba8n5JB6YZ7+hzl+irLToR9omS/jDg923qrvneQ9JvbL9ie36nmxnE+IjYUVveKWl8J5sZRMNpvNvpc9OMd81z18z050XxAd0XzYiI0ySdI+mq2svVrhT978G6aex0SNN4t8sg04z/WSefu2anPy+qE2HfLun4Ab8fV1vXFSJie+12t6S16r6pqHcdmEG3dru7w/38WTdN4z3YNOPqgueuk9OfdyLs6yVNtv012yMlfV/Skx3o4wtsH1H74ES2j5D0HXXfVNRPSrq8tny5pCc62MtBumUa73rTjKvDz13Hpz+PiLb/SJql/k/k/1vSv3Sihzp9/bWkV2s/r3W6N0mPqP9l3Z/U/9nGDyX9laTnJL0p6T8lHd1FvT0kabOkTeoP1oQO9TZD/S/RN0naWPuZ1ennLtFXW543vi4LZIIP6IBMEHYgE4QdyARhBzJB2IFMEHYgE4QdyMT/A9Er82Ffo8RDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#collapse\n",
    "test_inp = x_valid[0]\n",
    "plt.title(\"Input: \")\n",
    "plt.imshow(test_inp.reshape(28,28), cmap=\"binary\");\n",
    "\n",
    "loss, pred = nn.forward(test_inp)\n",
    "predicted_val = int(pred > 0.5)\n",
    "print(f\"Predicted output: {predicted_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#collapse\n",
    "test_inp = x_valid[2000]\n",
    "plt.title(\"Input: \")\n",
    "plt.imshow(test_inp.reshape(28,28), cmap=\"binary\");\n",
    "\n",
    "loss, pred = nn.forward(test_inp)\n",
    "predicted_val = int(pred > 0.5)\n",
    "print(f\"Predicted output: {predicted_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We were able to create a model that can identify classify handwritten digits as either 1's or 0's\n",
    "- We successfully computed the `forward` and `backward` progation of a `neural network` from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thanks for reading !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
